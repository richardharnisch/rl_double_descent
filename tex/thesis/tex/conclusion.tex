\section{Conclusion}\label{sec:conclusion}
% TODO: ask if i even need a conclusion?
Our investigation into the existence of Double Descent in Reinforcement Learning has yielded intriguing insights, albeit without definitive evidence of the phenomenon under the conditions tested. While we observed certain dynamics in model performance and Fisher information, these did not align with the expected patterns of Double Descent as documented in supervised learning contexts. Notably, the absence of a clear interpolation threshold and the nonstationary nature of RL data can play significant roles in shaping generalization behavior differently than in supervised settings. Furthermore, the architectural choices, particularly the use of MLPs for spatial tasks, may have limited the models' ability to generalize effectively. Our findings do not preclude the existence of Double Descent in RL, but rather motivate further investigation of Double Descent within Reinforcement Learning. Future research, as described in Section \ref{sec:limitations-and-future-work}, is essential to clarify the conditions under which Double Descent might manifest in Reinforcement Learning.