\section{Introduction}\label{sec:introduction}
Double Descent (DD) has been observed in supervised learning (SL) \citep{Belkin_2019,loog2020briefprehistorydoubledescent}. When increasing model capacity, training steps, or dataset size, the test risk of deep models begins to improve (decrease) with the train risk up until a ``sweet spot" at which test risk is traditionally understood to be minimized \citep[p.~221]{hastie01statisticallearning}. Past this point, increasing model capacity, training steps, or dataset size leads to overfitting, a behavior where the model memorizes the training data and thus fails to generalize over unseen test data. In other words, a generalization gap grows between the test performance and the train performance. This behavior can be visualized as a U-shaped risk curve, which can be seen on the left of Figure~\ref{fig:double_descent}. However, recent work has shown that this classical regime does not always persist. After this initial overfitting phase, increasing model capacity, training steps, or dataset size further leads to a second descent in test risk, as can be seen on the right of Figure~\ref{fig:double_descent}. This phenomenon has been coined ``Double Descent" (DD) and has been observed across various architectures and datasets in SL \citep{nakkiran2019deepdoubledescentbigger, hastie2020surpriseshighdimensionalridgelesssquares}.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{../images/doubledescent.pdf}
    \caption{DD risk curve. After the initial overfitting phase, increasing model capacity, training steps, or dataset size further leads to a second descent in test risk. Figure taken from \cite{Belkin_2019}.}
    \label{fig:double_descent}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{../images/nakkiran_double_descent.png}
    \caption{DD observed in practice in SL on CIFAR-10 with ResNets, varying training epochs (vertical axis), model capacity (horizontal axis), and test risk (color coding). The figure shows Double Descent occurs under both regimes. Figure taken from \cite{nakkiran2019deepdoubledescentbigger}.}
    \label{fig:nakkiran_dd}
\end{figure}

\subsection{Literature Review: DD in SL} % are these enough citations? not sure
In SL, non-monotonic generalization behavior as a function of model complexity has been discussed in various forms for decades, with renewed attention focused on DD in recent years \citep{loog2020briefprehistorydoubledescent}. For instance, \cite{boes1996dynamics} reported that when model size is close to the number of training samples, optimization can slow substantially when training single layer perceptrons to a fixed test risk threshold. The recent wave of interest was strongly influenced by \cite{Belkin_2019}, who provide a clear empirical demonstration of DD across various architectures and datasets in the capacity regime. They conduct an empirical study of DD using neural networks and decision trees on the MNIST database \citep{lecun1998gradientbasedlearningapplieddocumentrecognition} as well as other real and synthetic datasets, showing that increasing the capacity of the tested models beyond that at which zero train risk is achieved leads to an ascent and then descent in terms of the test risk, as shown in Figure~\ref{fig:double_descent}. The authors coin the term ``Double Descent" to refer to this phenomenon, and posit that it occurs at the ``interpolation threshold", which is the point at which model capacity (number of parameters) is equal to the size of the train set. Expanding upon this work, \cite{nakkiran2019deepdoubledescentbigger} conduct a large-scale empirical study to provide further evidence that DD is a widespread in SL. The authors demonstrate that DD occurs not only with regard to the capacity regime but also with regard to the episodic regime (the amount of training steps), as well as that test risk as a function of dataset size can also exhibit non-monotonic behavior. They define a measure of ``Effective Model Capacity" (EMC), the number of training samples a model can learn to zero train risk under a given model and training configuration, and posit that the interpolation threshold is at this EMC, determined not only by parameter count. Figure~\ref{fig:nakkiran_dd}, taken from their paper, shows test risk as a function of both model capacity and training epochs, using convolutional neural networks of varying widths trained on an image classification task. DD is visible under both the capacity and the episodic regimes. They emphasize the role of label noise in amplifying the effect, showing that DD is more pronounced when there is label noise in the training data. In the noiseless case, they often observe a `plateau' rather than a peak in the test risk at the interpolation threshold. Later, \cite{nakkiran2019datahurtlinearregression} shows more clearly that DD can occur in the dataset size regime using linear regression models. \cite{dAscoli2021TripleDescent} further investigate DD and find that peaks in the test risk can occur at two distinct points. Given the number of training samples $N$, the number of model parameters  $P$, and the model's input dimension $D$, the authors find that the test risk can peak at both the interpolation threshold $N \approx P$ and at the ``classical" threshold $N \approx D$, leading to a ``Triple Descent" curve when plotting test risk against $N$. The curve exhibits its first descent in the regular underparameterized regime ($N < D$), followed by a peak at $N \approx D$, then a second descent in the overparameterized but non-interpolating regime ($D < N < P$), and finally a third peak at the interpolation threshold $N \approx P$, before descending again in the highly overparameterized regime ($N > P$). However, recent work has questioned the ubiquity of DD in SL. \cite{Buschjger2021ThereIsNoDoubleDescentRandomForests} replicates the random forest experiments from \cite{Belkin_2019} and finds that, when controlling for the number of trees and considering total node count, the test risk decreases monotonically with model size, showing no evidence of DD in this setting. Furthermore, \cite{curth2023uturndoubledescent} argue that much of the reported ``DD" in classical (non-deep) models arises because the plotted parameter count implicitly follows multiple distinct complexity directions, and the ``second descent" appears when experiments switch from one way of adding parameters to another, so its location is not inherently tied to when the capacity matches the training data. They further propose an effective-parameter view, under which the apparent DD curves can be reconciled with more traditional U-shaped behavior.

The underlying mechanisms behind DD are still debated, but several complementary hypotheses have emerged in the SL literature. One explanation holds that the interpolation threshold creates a variance spike in the learned function \citep{Belkin_2019}. As the model approaches the capacity to fit the training data exactly, it must contort its decision boundary or function representation to accommodate all training points, including noise or outliers. This leads to high variance in predictions on unseen data, harming generalization and producing the peak in test risk. Adding more parameters allows the model to fit the data more smoothly, reducing variance and leading to the second descent. In linear regression, this can explicitly be shown to be due to the variance of the estimator diverging at the interpolation threshold \citep{hastie2020surpriseshighdimensionalridgelesssquares}. Additionally, even when models are large enough to interpolate, training algorithms such as stochastic gradient descent do not pick arbitrarily from the set of parameter vectors that can achieve zero train risk. Instead, optimizers like stochastic gradient descent tend to select solutions with particular simplicity biases, such as small norms of the parameter vector \citep{gunasekar2017implicitregularizationmatrixfactorization, li2018algorithmicregularizationoverparameterized}. From this view, the second descent occurs because, with sufficient capacity, the optimizer can find interpolating solutions that also have smaller norms \citep{Belkin_2019}. \cite{nakkiran2019deepdoubledescentbigger} also show DD is often most pronounced when there is irreducible noise in the targets (e.g., label noise). In that case, interpolating the train set requires fitting noise as well as signal. Near the interpolation threshold, the model begins to fit this noise, harming generalization and producing the peak. With further overparameterization (and sufficient training), the model can fit noise in a way that is less damaging to the learned representation, yielding the second descent.

\subsection{Prior Work on DD in RL}
The existence of generalization gaps in RL is known for some time. \cite{zhang2018studyoverfittingdeepreinforcement} show that generalization gaps can emerge in RL when agents are trained on a set of train environments and evaluated on a held-out set of test environments, using gridworld maze environments randomly generated based on a seed. The authors find the generalization gap diminishes as the train set size increases but are unable to demonstrate a U-shaped curve in test performance as a function of training steps, with training return instead stagnating at a suboptimal level. \cite{cobbe2019quantifyinggeneralizationreinforcement} also use procedurally generated environments to show that RL agents can overfit to a surprisingly large number of training environments, and that training performance can increase with training set size, which they attribute to an implicit curriculum effect. They use models of three capacities and find that larger models generalize better on their environments. \cite{mediratta2024gengap} test generalization gaps in offline RL and find that the generalization gap is smaller as the model's capacity increases. They also find overfitting in the episodic regime analogous to the ``classical regime'' in SL, noting that extended training yields further benefits in train performance but hurts test performance.

While generalization gaps are well documented, the status of DD in RL is less clear and appears to depend on the learning setup and on what is used as a proxy for generalization. \cite{brellmann2024doubledescentreinforcementlearning} provide a theoretical account of DD in an RL context by studying policy evaluation with the regularized Least-Square Temporal Difference algorithm and random features. In a regime where the number of parameters $N$ and the number of distinct visited states $m$ grow proportionally, they derive limiting expressions for the mean-squared Bellman error and show that DD-like behavior is governed by the ratio $N/m$, with a peak near the interpolation threshold $N/m \approx 1$, and that stronger $\ell_2$ regularization and improved state coverage attenuate or remove the effect. Together, these works suggest that DD can arise in RL under specific conditions. \cite{vesel√Ω2025presencedoubledescentdeepreinforcement} investigate DD in deep model-free RL by varying network capacity and analyze DD-like behavior through an information-theoretic lens, using policy entropy as a central diagnostic. They report that DD-style patterns can emerge in certain actor--critic settings, but emphasize that standard RL objectives may be unreliable indicators of generalization and call for evaluation on explicit out-of-distribution benchmarks, similar to the contributions of our work.