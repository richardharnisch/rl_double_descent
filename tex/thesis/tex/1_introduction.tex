% TODO: in general rewrite the intro to fit around the new literature review

\section{Introduction}\label{sec:introduction}
% TODO: citations!!
Double Descent (DD) is a phenomenon in supervised learning (SL). When increasing model capacity, training time, or dataset size, the test risk of deep models begins to improve (decrease) with the train risk up until a "sweet spot" at which test risk is traditionally understood to be minimized. Past this point, increasing model capacity or training time leads to overfitting, a behavior where the model memorizes the training data and thus fails to generalize over unseen test data. In other words, a generalization gap grows between the test performance and the train performance. This behavior can be visualized as a U-shaped risk curve, which can be seen on the left of Figure~\ref{fig:double_descent}. However, work % TODO: ONE MILLION CITATIONS!!!!!!!!!!!
in the past years has shown that this traditional regime can be escaped. After this initial overfitting phase, increasing model capacity, training time, or dataset size further leads to a second descent in test risk \citep{Belkin_2019,nakkiran2019deepdoubledescentbigger,loog2020briefprehistorydoubledescent}, as can be seen on the right of Figure~\ref{fig:double_descent}. This phenomenon has been coined "Double Descent" (DD) and has been observed across various architectures and datasets in SL \citep{nakkiran2019deepdoubledescentbigger}. % TODO: explain what is in the papers i cite!

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{../images/doubledescent.pdf}
    \caption{DD risk curve. After the initial overfitting phase, increasing model capacity, training time, or dataset size further leads to a second descent in test risk. Figure taken from \cite{Belkin_2019}.}
    \label{fig:double_descent}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{../images/nakkiran_double_descent.png}
    \caption{DD observed in practice in SL on CIFAR-10 with ResNets, varying training epochs (vertical axis), model capacity (horizontal axis), and test risk (color coding). The figure shows Double Descent occurs under both regimes. Figure taken from \cite{nakkiran2019deepdoubledescentbigger}.}
    \label{fig:nakkiran_dd}
\end{figure}

\subsection{Literature Review: DD in SL}
In SL, non-monotonic generalization behavior as a function of model complexity has been discussed in various forms for decades, with renewed attention focused on DD in recent years \citep{loog2020briefprehistorydoubledescent}. For instance, \cite{boes1996dynamics} reported that when model size is close to the number of training samples, optimization can slow substantially when training single layer perceptrons to a fixed test-risk threshold. The recent wave of interest was strongly influenced by \cite{Belkin_2019}, who provide a clear empirical demonstration of DD across various architectures and datasets in the capacity regime. They conduct an empirical study of DD using neural networks and decision trees on the MNIST database \citep{deng2012mnist} as well as other real and synthetic datasets which shows that increasing the capacity of the tested models beyond that at which zero train risk is achieved leads to an ascent and then descent in terms of the test risk, as shown in Figure~\ref{fig:double_descent}. They coin the term "Double Descent" to refer to this phenomenon, and posit that it occurs at the "interpolation threshold", which is the point at which model capacity (number of parameters) is equal to the size of the train set. Expanding upon this work, \cite{nakkiran2019deepdoubledescentbigger} show that DD can occur in both the capacity and the episodic regimes. They conduct a large-scale empirical study of DD across various architectures to show that DD is a widespread phenomenon in SL, and also show that it applies to not only the capacity regime but also the episodic regime, as well as showing that test risk as a function of dataset size can also exhibit non-monotonic behavior. They define a measure of "Effective Model Capacity" (EMC), the number of training samples a model can learn to zero train risk under a given model and training configuration, and posit that the interpolation threshold is at this EMC, determined not only by model parameter count. Figure~\ref{fig:nakkiran_dd} from their paper shows test risk as a function of both model capacity and training epochs, using convolutional neural networks of varying widths trained on an image classification task. DD is visible under both the capacity and the episodic regimes. They also emphasize the role of label noise in amplifying the effect, showing that DD is more pronounced when there is label noise in the training data. Later, \cite{nakkiran2019datahurtlinearregression} shows more clearly that DD can occur in the dataset size regime using linear regression models. \cite{dAscoli2021TripleDescent} further investigates DD and finds that peaks in the test risk can occur at two distinct points. They define the number of training samples $N$, the number of model parameters  $P$, and the model's input dimension $D$. They find that the test risk can peak at both the interpolation threshold $N \approx P$ and at the "classical" threshold $N \approx D$, leading to a "Triple Descent" curve when plotting test risk against $N$. The curve exhibits its first descent in the regular underparameterized regime ($N < D$), followed by a peak at $N \approx D$, then a second descent in the overparameterized but non-interpolating regime ($D < N < P$), and finally a third peak at the interpolation threshold $N \approx P$, before descending again in the highly overparameterized regime ($N > P$). However, recent work has questioned the ubiquity of DD in SL. \cite{Buschjger2021ThereIsNoDoubleDescentRandomForests} replicates the random forest experiments from \cite{Belkin_2019} and finds that, when controlling for the number of trees and considering total node count, the test risk decreases monotonically with model size, showing no evidence of DD in this setting. Furthermore, \cite{curth2023uturndoubledescent} argue that much of the reported "DD" in classical (non-deep) models arises because the plotted parameter count implicitly follows multiple distinct complexity directions, and the "second descent" appears when experiments switch from one way of adding parameters to another, so its location is not inherently tied to $p=n$. They further propose an effective-parameter view, under which the apparent DD curves can be reconciled with more traditional U-shaped behavior.

The underlying mechanisms behind DD are still debated, but several complementary hypotheses have emerged in the SL literature. One explanation is that the interpolation threshold creates a variance spike in the learned function. As the model approaches the capacity to fit the training data exactly, it must contort its decision boundary or function representation to accommodate all training points, including noise or outliers. This leads to high variance in predictions on unseen data, harming generalization and producing the peak in test risk \citep{Belkin_2019}. Adding more parameters allows the model to fit the data more smoothly, reducing variance and leading to the second descent. In linear regression, this can be shown explicitly because the variance of the estimator diverges at the interpolation threshold \citep{hastie2020surpriseshighdimensionalridgelesssquares}. Additionally, even when models are large enough to interpolate, training algorithms such as stochastic gradient descent do not pick arbitrarily from the set of pararameter vectors that can achieve zero train risk. Instead, optimizers like stochastic gradient descent tend to select solutions with particular simplicity biases, such as small norms of the parameter vector, which can generalize well in overparameterized settings. From this view, the second descent occurs because, with sufficient capacity, the optimizer can find interpolating solutions that also satisfy these regularizations \citep{Belkin_2019}. DD is often most pronounced when there is irreducible noise in the targets (e.g., label noise). In that case, interpolating the train set requires fitting noise as well as signal. Near the interpolation threshold, the model begins to fit this noise, harming generalization and producing the peak. With further overparameterization (and sufficient training), the model can fit noise in a way that is less damaging to the learned representation, yielding the second descent \citep{nakkiran2019deepdoubledescentbigger}.

\subsection{Prior Work on DD in RL} % TODO: ask why "RL" is circled, ask about the asterisk between the two sections
While DD % TODO: ask why "DD" is circled
is well documented in SL, its status in RL is less clear and appears to depend on the learning setup and on what is used as a proxy for generalization.
\cite{brellmann2024doubledescentreinforcementlearning} provide a theoretical account of DD in an RL context by studying policy evaluation with regularized LSTD and random features. In a regime where the number of parameters $N$ and the number of distinct visited states $m$ grow proportionally, they derive limiting expressions for the mean-squared Bellman error and show that DD-like behavior is governed by the ratio $N/m$, with a peak near the interpolation threshold $N/m \approx 1$, and that stronger $\ell_2$ regularization and improved state coverage attenuate or remove the effect. Together, these works suggest that DD can arise in RL under specific conditions. \cite{vesel√Ω2025presencedoubledescentdeepreinforcement} investigate DD in deep model-free RL by varying network capacity and analyze DD-like behavior through an information-theoretic lens, using policy entropy as a central diagnostic. They report that DD-style patterns can emerge in certain actor--critic settings, but emphasize that standard RL objectives may be unreliable indicators of generalization and call for evaluation on explicit out-of-distribution benchmarks, similar to the contributions of our work.