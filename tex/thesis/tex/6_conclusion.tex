\section{Conclusion}\label{sec:conclusion}
% TODO: merge with discussion and limitations/future work sections
Our investigation into the existence of DD in RL has not yielded definitive evidence of the phenomenon under the conditions tested. While we observed a generalization gap emerging between train and test return, this did not close upon increasing the model capacity or the training episodes.

Notably, the absence of a clear interpolation threshold and the nonstationary nature of RL data can play significant roles in shaping generalization behavior differently than in supervised settings. Furthermore, the architectural choices, particularly the use of MLPs for spatial tasks, may have limited the models' ability to generalize effectively. Our findings do not preclude the existence of DD in RL, but rather motivate further investigation of DD within RL.