\section{Discussion and Future Work}\label{sec:discussion}
In general, we are unable to observe DD in our experiments after testing for both the episodic capacity and regimes. Initially, both train and test returns increase with model capacity and training episodes. After a certain point, the test return stagnates while the train return continues to increase, leading to a widening generalization gap, which does not close under any of our tested configurations. The same occurs in the episodic regime. This suggests that DD does not occur under the conditions that we tested. While it is impossible to determine a precise reason for this, we can speculate on a range of possible, potentially complementary explanations.
First, there is no clear analogue of the interpolation threshold. A key ingredient for DD is traversing a regime where the model can fit the train set. In our experiments, train return for TRPO often saturates below the environment's optimal return. If the policy does not reliably solve the training distribution, then we may never reach a true interpolation-like regime in which DD is expected to appear; instead, increasing capacity remains in the partial fitting regime without transitioning into a qualitatively different overparameterized regime. To attempt to reach perfect return on the training maps, we could make two main changes in the model. We could increase the number of parameters and training episodes further, which is currently limited by our computational resources, or we could change the architecture of our model from a MLP to a convolutional neural network (CNN) \citep{LeCun2015}, which is better suited to our spatial task and observation space. In this case, we would process an $8\times 8$ grid input with 8 channels (four object types and frame-stacking of two frames).
Secondly, unlike SL, RL data is policy-induced and nonstationary. As the policy improves, the state--action visitation distribution changes. This violates the usual fixed distribution that is the case in SL. As a result, the relationship between capacity and generalization may be dominated by exploration and visitation effects (what data are collected) rather than interpolation effects (how well a fixed dataset can be fit). In future work, this could be addressed by training an off-policy method on a fixed dataset of trajectories collected on a set of training maps, and then plotting performance of the trained model on held-out test maps as a function of capacity. This would be more analogous to the SL setting and would allow us to test whether DD can appear in RL when the data distribution is fixed. It would be necessary to ensure that the chosen off-policy method can reliably fit the training data, which we struggled to achieve using a DQN.

With regard to the average Fisher information, we observe an initial spike as the model begins to memorize the training data, after which it decreases again. This would usually suggest that the model is finding a more generalizable policy after the initial memorization phase, corresponding to a flatter gradient landscape. However, the decrease in average Fisher information does not correspond to an improvement in test return, which remains stagnant after the initial increase. This decoupling of Fisher information dynamics and generalization capacity is puzzling and warrants further investigation to understand the underlying mechanisms at play. One way to further investigate this is by measuring the effective rank of the FIM along with its trace, which would give insight into how many dimensions of high sensitivity the model has and how this changes with capacity. It would also be interesting to examine the state visitation distribution across training runs and model capacities to see if it shrinks as capacity increases, which would support the hypothesis that the model is becoming more deterministic and thus visiting fewer states. Another signal of the policy becoming more deterministic would be a decrease in the entropy of the policy distribution, which could be measured across model capacities.

% TODO: expand, expand, expand! how would you test this? what would you expect to see if this is true? what are the implications for using the average Fisher information as a measure of generalization capacity in RL?

In the episodic regime, we observe a slight downward trend in test return as we increase the number of training episodes, after an initial increase. This could be interpreted as overfitting manifesting itself in a degradation of test return with more training. This trend emerges repeatedly across different train set sizes (50, 100) and different model capacities (3 depth and 16, 64, 256, 1024 widths). However, the effect is quite small and needs to be investigated further, such as by continuing training runs into the order of tens or hundreds of millions of episodes, both to see for how long this trend continues and to see whether it eventually reverses into increasing average return in the episodic regime at extremely high episode counts. It is possible that a clear DD curve only emerges when plotting on a logarithmic scale with regard to episodes. Additionally, it would be interesting to also measure effective rank of the FIM in addition to its trace in the episodic regime, to see if it decreases over training.

% TODO: how?

Considering DD is strongest in SL when irreducible noise is present in the data, adding stochasticity to the environment could create conditions more conducive to observing DD phenomena. The same effect could possibly be achieved by using a partially observable MDP (POMDP) setup, where the agent only receives limited observations of the environment state rather than the full grid. This would introduce uncertainty and require the agent to generalize from incomplete information, potentially influencing the emergence of DD behavior.

One main limitation we face is the availability of computational resources, which limited the number of configurations we could test and forced us to search the hyperparameter space in a heuristic manner. Future work could leverage greater computational resources to employ a grid search over the train set size, model capacity, and number of training episodes exhaustively. Concretely, it would be informative to create a similar plot for each train set size as \cite{nakkiran2019deepdoubledescentbigger} did in Figure~\ref{fig:nakkiran_dd}.

In this work, we examined generalization behavior of RL agents across different model capacities and training regimes in an effort to show DD. Under both the episodic and capacity regimes, we were unable to observe a clear DD curve, with the generalization gap remaining mostly constant with increased capacity and training results. Overall, our results suggest that DD may not be a universal phenomenon in RL.