\section{Discussion and Future Work}\label{sec:discussion}
In general, we are unable to observe DD in our experiments under both the capacity and episodic regimes. Initially, both train and test returns increase with both model capacity and training episodes. After a certain point, the test return stagnates while the train return continues to increase, leading to a widening generalization gap which does not close under any of our tested configurations. This suggests that DD does not occur under the conditions that we tested. We propose two possible, potentially complementary explanations.

First, there is no clear analogue of the interpolation threshold. A key ingredient for DD is traversing a regime where the model can fit the train set. In our experiments, train return for TRPO often saturates below the environment's maximum return. If the policy does not reliably solve the training distribution, then we may never reach a true interpolation-like regime in which DD is expected to appear; instead, increasing capacity remains in the partial fitting regime without transitioning into a qualitatively different overparameterized regime. Almost all of our models saturate below maximum return on their training sets, indicating they have not crossed the interpolation threshold. In contrast with \cite{brellmann2024doubledescentreinforcementlearning}, who clearly define an interpolation threshold, the transitions that our models experience are not controlled and we are therefore unable to define a clear analogue. Becuase our data is nonstationary and policy-induced, it may be that the interpolation threshold is a moving target fluctuating within a certain range. This could manifest itself with a slow transition from underfitting to overfitting, as the model capacity gradually grows within and past this range, which could proclude the variance spike caused by the interpolation threshold in SL. To attempt to reach maximum return, two main changes in the model are possible. We could increase the number of parameters and training episodes further, which are currently limited by our computational resources, or we could change the architecture of our model from a MLP to a convolutional neural network (CNN) \citep{lecun1989handwrittendigitrecognitionbackpropagation}, which is better suited to our spatial task. In this case, we would process an $8\times 8$ grid input with 8 channels (four object types and frame-stacking of two observations).

Secondly, unlike SL, RL data is policy-induced and nonstationary. As the policy improves, the state--action visitation distribution changes. This violates the usual fixed distribution that is an assumption in SL. As a result, the relationship between capacity and generalization may be dominated by exploration and visitation effects (what data are collected) rather than interpolation effects (how well the dataset can be fit). In future work, this could be addressed by training an off-policy method on a fixed dataset of trajectories collected on a set of training maps, and then plotting performance of the trained model on held-out test maps as a function of capacity. This would be more analogous to the SL setting and would allow us to test whether DD can appear in RL when the data distribution is fixed. It is necessary to ensure that the chosen off-policy method can reliably fit the training data, which we fail to achieve using a DQN. % todo: how is this not just SL?

We observe an initial spike in average Fisher information as the model begins to memorize the training data. However, the spike does not persist and the average Fisher information returns to a low range. This would usually suggest that the model is finding a more generalizable policy after the initial memorization phase, corresponding to a flatter gradient landscape. However, the decrease in average Fisher information does not manifest itself in an improvement in test return, which remains stagnant after the initial increase. This decoupling of Fisher information and generalization capacity is puzzling and warrants further investigation. One issue is that high average Fisher information could reflect a few dimensions of extreme sensitivity, indicating overfitting, or many dimensions of moderate sensitivity, which may be less problematic. To disentangle these possibilities, measuring the effective rank of the FIM along with its trace could provide insight into how many dimensions of high sensitivity the model has and how this changes with capacity. If the effective rank is low, it would suggest that the model is overfitting by relying heavily on a few sensitive directions. If the effective rank is high, it would suggest that the model is more robust and generalizable, even if the average Fisher information is high. However, such analysis would still face the nonstationarity issue, as the FIM is computed on the train set and may not reflect sensitivity on the test set, which is where generalization is evaluated. Examining the state visitation distribution across training runs and model capacities could reveal whether it shrinks as capacity increases, which would support the hypothesis that the model is becoming more deterministic and thus visiting fewer states. Another signal of the policy becoming more deterministic would be a decrease in the entropy of the policy distribution, which could be measured across model capacities to see if it correlates with changes in generalization performance.

In the episodic regime, after an initial increase we observe a slight downward trend in test return as we increase the number of training episodes. This could be interpreted as overfitting manifesting itself in a degradation of test return with more training. This trend emerges repeatedly across different train set sizes (50, 100) and different model capacities (widths 16, 64, 256, 1024), and it is consistent with findings reported by \cite{mediratta2024gengap}. However, the effect is quite small and warrants further investigation, such as by continuing training runs into the order of tens or hundreds of millions of episodes, both to observe for how long this trend continues and whether it eventually reverses into an increase in average return. It is possible that a clear DD curve only emerges when plotting on a logarithmic scale on the episodic axis. Again, it would be interesting to also measure effective rank of the FIM in addition to its trace in the episodic regime, to see if it decreases over training, which could support that the model is overfitting to the training set.

Considering DD is strongest in SL when irreducible noise is present in the data, adding stochasticity to the environment could create conditions more conducive to observing DD phenomena. This could be achieved by adding a degree of stickiness to the actions, where the last action can be repeated in the next state regardless of the agent's choice, or by making the environment ``slippery'' by introducing a probability that a movement direction adjacent to the chosen one is executed. The same effect may also be achieved by using a partially observable MDP (POMDP) setup, where the agent only receives limited observations of the environment state rather than the full grid. This would introduce uncertainty and require the agent to generalize from incomplete information, potentially influencing the emergence of DD behavior.

The main limitation we face is the availability of computational resources, which limits the number of configurations we could test and forced us to search the hyperparameter space in a heuristic manner. Future work could leverage greater computational resources to employ a grid search over the train set size, model capacity, and number of training episodes exhaustively. Concretely, it would be informative to create a similar plot for each train set size as \cite{nakkiran2019deepdoubledescentbigger} did in Figure~\ref{fig:nakkiran_dd}.

In this work, we examined generalization behavior of RL agents across different model capacities and training regimes in an effort to show DD. Under both the episodic and capacity regimes, we were unable to observe a clear DD curve, with the generalization gap persisting and failing to close under increased capacity or training duration. Overall, our results suggest that DD may not be a universal phenomenon in RL.