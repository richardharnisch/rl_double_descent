\begin{abstract}
    {Double Descent (DD) is a test-risk phenomenon in which a deep model's test risk on a test set worsens around the interpolation threshold (overfitting) but then improves again when increasing training episodes, model size, or dataset size. We implement an empirical experiment to test whether this phenomenon can appear in Reinforcement Learning (RL). To this end, we train Deep Q-Network (DQN) and Trust-Region Policy Optimization (TRPO) agents on a family of seeded grid-worlds with obstacles and evaluate the agent on held-out maps. Across model sizes and training duration, we observe a generalization gap appearing between returns on training and held-out test maps. However, upon increasing the size of the model as well as the amount of training timesteps we do not observe a second descent regime in test risk. The results suggest that in this context, increased capacity and training do not recover generalization and motivate further experiments with different algorithms, architectures, and environment types.\vspace{6ex}}
\end{abstract}
