\section{Discussion}\label{sec:discussion}

In general, we are unable to observe DD in our experiments. We tested for both forms of DD: the episodic and capacity regime, but neither produced a closing of the generalization gap between train and test performance. Initially, both performances increase with model capacity and training time, but after a certain point, the test performance stagnates while the train performance continues to increase, leading to a widening generalization gap. As we increase the training set size, we observe the generalization gap shrinks, but do not observe a change in the dynamics. % TODO: which dynamics
This suggests that DD does not occur under the conditions that we tested.

With regard to the average Fisher information, we observe an initial spike as the model begins to memorize the training data, after which it decreases again. This is an intriguing phenomenon, as it indicates that while the model initially focuses on fitting the training data closely (leading to high sensitivity to parameter changes), it subsequently adjusts its parameters in a way that reduces this sensitivity. This behavior could suggest that the model is finding a more robust representation of the data after the initial memorization phase. However, the decrease in average Fisher information does not correspond to an improvement in test performance, which remains stagnant after the initial increase. This decoupling of Fisher information dynamics and generalization performance is puzzling and warrants further investigation to understand the underlying mechanisms at play. % TODO: expand, expand, expand! how would you do it? give some suggestions on how to investigate this further, what would you look for, what would you expect to find if your hypothesis is correct.

In the episodic regime, we observe a slight downward trend in test performance as we increase the number of training episodes, after an initial increase. This could be interpreted as overfitting manifesting itself in a degradation of test performance with more training. This trend emerges repeatedly across different training set sizes (50, 100) and different model capacities (3 depth and 16, 64, 256, 1024 widths). However, the effect is quite small and needs to be investigated further. We also fail to see consistent trends in the average Fisher information in the episodic regime, with values seemingly dominated by noise.

Overall, our results suggest that DD may not be a universal phenomenon in RL, especially under the conditions we tested. It is impossible to determine a precise reason for this, considering the complexity of RL. However, we can speculate on a range of possible, potentially complementary explanations.

First, there is no clear analogue of the interpolation threshold. A key ingredient for DD is traversing a regime where the model can (nearly) fit the training set. In our experiments, training performance for TRPO often saturates below the environment's near-optimal return, particularly as the number of training maps grows. If the policy does not reliably solve the training distribution, then we may never reach a true interpolation-like regime in which DD is expected to appear; instead, increasing capacity remains in the partial fitting regime without transitioning into a qualitatively different overparameterized regime.

Secondly, unlike SL, RL data is policy-induced and nonstationary. As the policy improves, the state--action visitation distribution changes. This violates the fixed-distribution assumption that underlies many DD intuitions. % TODO: "intuitions" is the wrong word here, i mean more like "conditions". explain which regular DD conditions are violated and why
As a result, the relationship between capacity and generalization may be dominated by exploration and visitation effects (what data are collected) rather than interpolation effects (how well a fixed dataset can be fit).

Lastly, our model takes a flattened grid as input, and both the policy and value networks are simple multi-layer perceptrons (MLPs). For spatial navigation, MLPs struggle to capture structure such as locality or translation invariance, which convolutional or graph-based models handle better (see \cite{cohen2016groupequivariantconvolutionalnetworks}). As a result, making the MLP larger mainly improves memorization of the training maps rather than generalization to new maps. Test performance therefore plateaus even as training performance improves, so we do not observe the "second descent" that DD predicts.

The dynamics of the average Fisher information also present an interesting avenue for further research, particularly in understanding its relationship with generalization performance. The initial increase of the average Fisher information during the memorization phase suggests that the model becomes highly sensitive to parameter changes as it fits the training data closely. This is expected as this represents a regime where the model is likely overfitting to the training data, which we can observe in the generalization gap between train and test performance. However, the subsequent decrease in average Fisher information, despite stagnant test performance, indicates that the model may be finding a more stable parameter configuration that is less sensitive to perturbations. This decoupling of Fisher information dynamics and generalization performance is puzzling and warrants further investigation to understand the underlying mechanisms at play.

One possible cause for this lowering of the average Fisher information despite the remaining stagnant test performance is that the Fisher information we estimate is fundamentally on-policy and therefore tightly coupled to the state--action visitation distribution induced by the current policy. Concretely, the empirical Fisher information matrix for a policy $\pi_\theta(a \mid s)$ is an expectation of score outer products, as defined in Equation \eqref{eq:fim_definition}. As training progresses, both the distributions of the states seen and the actions taken in any given state changes. This means that a decrease in the average Fisher information can reflect a change in what states are visited and how stochastic the policy is on those states, rather than an improvement in out-of-distribution generalization.

A plausible mechanism is increasing policy determinism. As TRPO improves on the training maps, the policy often becomes more confident, concentrating probability mass on a small set of actions. This can reduce the magnitude of $\nabla_\theta \log \pi_\theta(a \mid s)$ for on-policy actions, thereby reducing the empirical Fisher trace or mean diagonal. In other words, after an initial "memorization" phase with large, high-variance gradients (and thus a Fisher spike), the policy can enter a regime of saturated action preferences where the local sensitivity of the log-likelihood decreases even if the learned behavior remains brittle outside the training distribution. A similar dynamic can occur with a decreasing in the amount of states that are regularly visited. Importantly, this can coexist with poor performance on the test maps, as the policy can be locally stable and low-sensitivity on the training-induced state distribution while still failing catastrophically on unseen layouts that induce different state visitation.