\section{Limitations and Future Work}\label{sec:limitations-and-future-work}
% TODO: merge this section with discussion and conclusion section. lots of overlap.
Our study is subject to several limitations both in terms of scope and methodology, which point to avenues for future research.

First, we are constrained by the amount of computational resources available. Because experiment runs take multiple days when taking into account limited availability of GPUs and wait times in job queues on our high-performance cluster, we were only able to run a limited number of configurations. This forces us to search the hyperparameter space in a heuristic manner, relying on researcher intuition to select promising configurations rather than performing a systematic grid or random search. Future work could leverage greater computational resources to explore a wider range of hyperparameters, architectures, and training regimes more exhaustively. It would be especially interesting to explore the episodic regime into longer training runs, considering that we do observe a slight but consistent downward trend in test performance as we increase the number of training episodes.

Secondly, our experiments are limited to a single environment type and TRPO and DQN as RL algorithms. While GridWorld-like environments are a common benchmark for RL research, they may not capture the full complexity of more suitable tasks. Future studies could investigate DD in a variety of environments, including continuous control tasks or more complex navigation scenarios. Additionally, exploring other RL algorithms, such as investing more time into DQN or other value-based methods, could provide further insights into whether DD manifests differently across algorithmic paradigms. One straightforward extension would be to modify our environment to feature stochastic transitions or increase the frequency of obstacles, thereby increasing task complexity and potentially affecting generalization dynamics. Considering DD often relies on the presence of noise in SL, adding stochasticity to the environment could create conditions more conducive to observing DD phenomena. The same effect could possibly be achieved by using a partially observable MDP (POMDP) setup, where the agent only receives limited observations of the environment state rather than the full grid. This would introduce uncertainty and require the agent to generalize from incomplete information, potentially influencing the emergence of DD behavior.

Thirdly, it would be interesting to test architectures with stronger spatial inductive bias. The flattened grid input paired with an MLP may encourage memorization of idiosyncratic layouts rather than learning reusable spatial features. A straightforward extension is to replace the MLP with a convolutional policy/value network operating on the $8\times 8$ grid. If the generalization plateau is primarily an inductive-bias limitation, these models should raise test performance and may change the shape of generalization curves under capacity scaling.

Lastly, it could help our results to improve evaluation fidelity and report uncertainty. Our current evaluation reports only the mean return and average Fisher information. This can obscure subtler changes in the policy behavior. Future work should complement mean return with more diagnostic metrics such as success rate (fraction of episodes reaching the goal), steps-to-goal conditional on success, and the full return distribution over multiple rollouts with the stochastic policy. This would make it easier to find qualitatively different failure modes (e.g., policies that succeed rarely but very well versus policies that succeed often but only marginally). It would also be helpful to report return not as a raw number, but as a fraction of the optimal return (i.e., normalized between 0 and 1) on each tested map. This would make results more interpretable and comparable across different environments or reward scales. % TODO: regenerate graphs by making it a proportion of mean optimal performance rather than a raw return