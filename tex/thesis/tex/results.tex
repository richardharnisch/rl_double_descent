\section{Results}\label{sec:results}
In this section, we present the results of our experiments searching for DD in RL. Unfortunately, across all configurations tested, we did not observe instances of DD in either the episodic or capacity regime.

\subsection{DQN Fails to Learn}
First, we examined performance using DQNs on our environment. Unfortunately, this architecture was unable to solve the environment altogether. We observe frequent catastrophic forgetting, with spikes in mean return being present but short-lived. Examples of a DQN failing to learn the environment reliably can be found in Appendix \ref{sec:DQN-fails}, along with plots showing DQN mean return as a function of model capacity. These show that the failure to learn occurs over all tested model capacities. No DD is observed, as neither the training risk nor the test risk exhibit any significant learning. Similarly, the average Fisher information does not exhibit significant change and stays extremely close to zero across our tested model capacities. These results are uninformative for studying DD, as the model fails to learn the environment. Without a rise in train performance, a subsequent ascent in test performance cannot be assessed. Given the poor performance of DQNs on this environment, we therefore turned to an alternative architecture that we expected to perform more effectively.

\subsection{TRPO: No DD}
We next examined performance using TRPO. This architecture was able to learn the environment reliably across a range of model capacities. However, we were again unable to observe DD in either the episodic or capacity regimes.

\subsubsection{Capacity Regime}
\begin{figure*}[!hbtp]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/10seed_10k.png}
        \caption{Trained on 10 maps for 10,000 episodes.}
        \label{fig:trpo_capacity_10seed}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/500seed_1M.png}
        \caption{Trained on 500 maps for 1,000,000 episodes.}
        \label{fig:trpo_capacity_500seed}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/50seed_randcorner_20k.png}
        \caption{Trained on 50 maps for 20,000 episodes with randomized start and goal positions.}
        \label{fig:50seed_20k_random}
    \end{minipage}\hfill
\end{figure*}
In the capacity regime, we conducted studies to heuristically search the available hyperparameter space. Our two hyperparameters to calibrate here are size of the training set (in seeds) and amount of training time (in episodes). We varied both hyperparameters across a range of values, as shown in Table \ref{table:runs_capacity}. To plot the mean returns, we normalize by using the mean minimal and optimal returns across the seeds used for the specific configuration to scale the returns between 0 and 1. This makes the results more interpretable between different environment configurations.
First, we used 10 train maps and 10,000 training episodes per model, with the standard environment configuration. We can observe that both training and test performance are both low for the smallest model capacities. As we increase model capacity, training performance increases rapidly until it reaches the mean maximum return. Test performance increases slightly but stabilizes quickly at about 10\% of mean maximum return. No second ascent is observed in the test performance when increasing model capacity, and thus we cannot observe DD. At the same time, we observe a spike in the average Fisher information as the gap in mean return on the train and test set emerges. This is expected behaviour, as the model is likely to become more sensitive to parameter changes when it is overfitting on the train set. However, we observe the average Fisher information decreasing again as model capacity increases further. This is unexpected, as we would expect the model to remain sensitive to parameter changes after memorizing the training set. Figure \ref{fig:trpo_capacity_10seed} shows the results of this experiment. Considering the minimal improvement in test performance, we increased the training set size as well as the training episodes. We tested a training set size of 50 seeds and 20,000 training episodes, a training set size of 100 train maps and 50,000 episodes, and a training set size of 1,000 maps and 100,000 episodes. Here, we can observe similar results, with slightly improved test performance after the first ascent to about 0.2 of mean maximum return for 50 train maps and about 0.4 for 100 train maps. The training performance decreases with larger amounts of train maps, with mean return keeping around 0.8 for 50 train maps and 0.7 for 100 train maps. In these two configurations, no DD can be observed as the generalization gap does not close when increasing model capacity. As before, the average Fisher information again spikes as the train and test return diverges, but again decreases as model capacity increases further. When using 1,000 train maps, the train and test remain approximately equal and no generalization gap emerges at all, with both train and test return stabilizing at about 0.4 of mean maximum return.
Considering there is no generalization gap, we cannot observe DD. In this configuration we also cannot observe the spike in the average Fisher information that emerges with the generation gap. There is one high outlier value, but no sustained increase in the average Fisher information as in the other configurations. Plots showing these results can be found in Appendix \ref{sec:appendix_trpo}.

At this stage, there is no generalization gap, and thus further studying such configurations is not useful to our study of DD. To continue with large train sets, we need to allow the model to learn longer and reach higher training performance. We thus increased the amount of training episodes per model to 1,000,000. For this increased training time, we tested train set sizes of 100, 500, 750, and 1,000 maps. This longer training time shows the models reaching much closer to the optimal performance. However, even with this increased training time, we were again unable to observe DD in test performance. We see the generalization gap closing slowly with increased training set size, which is expected as the model sees more of the distribution of all possible maps and consistent with our results using fewer training episodes. The average Fisher information continues to spike as the model reaches its highest performance on the training set but lags behind in test performance, and decreases as model capacity increases further. An example of these results is shown in Figure \ref{fig:trpo_capacity_500seed}, with other configurations showing similar results in Appendix \ref{sec:appendix_trpo}.

Finally, on the configurations of 50 train maps and 20,000 training episodes per model as well as 100 train maps and 50,000 training episodes per model, we tested the randomized environment configuration, with both agent starting and goal position being selected randomly from the four corners of the map. This way the agent would be unable to use the basic rule of down and to the right to reach the goal. Here, we observe that mean return varies strongly with model capacity, specifically with the depth of the model, with deeper models performing worse than shallower models when keeping the width fixed. This may be related to optimization difficulties in deeper networks as they can be more difficult to train, as shown by \cite{he2015deepresiduallearningimage}. % todo: say something about how it is interesting this emerges especially with randomized configuration?
The test performance remains very low throughout all model capacities. This indicates that we would need to train on a larger set of train maps to see any generalization capacity whatsoever. The average Fisher information spikes approximately when the model reaches its highest performance on the training set, but then comes back down again. This is similar to our earlier findings of the average Fisher information spiking as the generalization gap emerges. The results are visualized in Figure \ref{fig:50seed_20k_random} and in Appendix \ref{sec:appendix_trpo}.

% TODO: discuss why the fim behaves like this

\subsection{Episodic Regime}\label{sec:episodic_regime}
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=\linewidth]{../images/100s_w1024.png}
    \caption{TRPO training and test performance over time for a model of width 1024 and depth 3, using 100 train maps and unlimited training episodes.}
    \label{fig:100seed_unlim_1024}
\end{figure}
DD can also occur with respect to the amount of optimization steps taken during training, which we refer to as the episodic regime. To test whether this regime exhibits DD under our conditions, we trained models of depth 3 and widths 4, 16, 64, 256, and 1024 on two configurations: 50 train maps and 100 train maps. Figure \ref{fig:100seed_unlim_1024} shows an example of the learning behavior during training. We trained without a limit on the number of training episodes, but in practice were limited to about 1.6M to 1.8M episodes until our high performance cluster ended the jobs. The remaining configurations exhibited similar behavior, except for the smallest models of width 4, which failed to learn the environment at all. These plots can be found in Appendix \ref{sec:longer-training-runs}.

Again, the results do not exhibit signs of DD. We can observe a quick increase in train return at the beginning, barely visible due to the scale of the graph. After this initial increase, training performance stays at a near-optimal level. The difference to the optimal level indicates that the model fails to solve a small number of maps or learned a suboptimal policy on maps it can solve. Test performance also increases quickly at the beginning and stabilizes at a low level between 0.2 and 0.4 for the tested configurations. However, we can observe a very slight downward trend in test performance over time, indicating that the model may be overfitting slightly to the training set. However, no second ascent is observed in the test performance within our training time, and thus no DD phenomenon. % TODO: rephrase last sentence, clarify DD vs DA