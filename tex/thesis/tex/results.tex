\section{Results}\label{sec:results}
% TODO: in general add fewer plots, and make them smaller. escape the double column env. four per row ideally
In this section, we present the results of our experiments searching for DD in Reinforcement Learning. Unfortunately, across all configurations tested, we did not observe instances of DDoding or capacity regimes.

\subsection{DQN Fails to Learn}
First, we examined performance using Deep Q-Networks (DQN) % TODO: no need to redefine what DQN is here
on our environment. Unfortunately, this architecture was unable to solve the environment altogether. We observe frequent catastrophic forgetting, with performance spikes being present but short-lived. Examples of a DQN failing to learn the environment reliably can be found in Appendix \ref{sec:DQN-fails}. This occurs over a range of different model capacities. Figure \ref{fig:dqn_capacity} shows metrics a range of model capacities, % TODO: rephrase
defined by varying width and depth of the neural network. No DD is observed, as neither curve exhibits significant learning. Similarly, the FIM trace does not show interesting movement, % TODO: rephrase
staying extremely close to zero throughout different model capacities.

\begin{figure}[!hbtp] % TODO: reformat *all* plots to have clear axes and legend labels, and to be properly legibile!
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/50seed_20k_dqn.png}
        \caption{Performance of DQN using 50 training seeds and 20,000 training episodes per model.}
        \label{fig:dqn_capacity_50seed}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/100seed_50k_dqn.png}
        \caption{Performance of DQN using 100 training seeds and 50,000 training episodes per model.}
        \label{fig:dqn_capacity_100seed}
    \end{subfigure}
    \caption{DQN training and test performance, and average Fisher information, as a function of model capacity. Neither curve exhibits significant learning.}
    \label{fig:dqn_capacity}
\end{figure}

These results are not useful to our study of DD, as the model fails to learn the environment in the first place. We cannot observe a second ascent of the test performance if there is no first ascent. % TODO: rephrase
Considering the poor performance of DQN on this environment, we decided to move to a different architecture that we hoped would perform better.

\subsection{TRPO: No DD}
We next examined performance using Trust Region Policy Optimization (TRPO). % TODO: do not redefine what TRPO is here!!!!!!!!!!
This architecture was able to learn the environment reliably across a range of model capacities. However, we were again unable to observe DD in either the episoding or capacity regimes.

\subsubsection{Capacity Regime}
In the capacity regime, we conducted studies to heuristically search the available hyperparameter space. Our two hyperparameters to calibrate here are size of the training set (in seeds) and amount of training time (in episodes). We varied both hyperparameters across a range of values, as shown in Table \ref{table:runs_capacity}. First, we examined performance using 10 training seeds and 10,000 training episodes per model, with the standard environment configuration. Figure \ref{fig:trpo_capacity_10seed} shows the results of this experiment. % make sure there is no newline here
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/10seed_10k.png}
    \caption{TRPO training and test performance, and average Fisher information, as a function of model capacity, using 10 training seeds and 10,000 training episodes per model.}
    \label{fig:trpo_capacity_10seed}
\end{figure}
We can observe that both training and test performance are low when the model is at its smallest. As we increase model capacity, training performance increases rapidly until it hits the maximum reward at approximately 0.96 mean return. Test performance increases slightly, stabilizing at approximately -0.5 mean return. No second ascent is observed in the test performance, and thus no DD. At the same time, we observe a spike in the average Fisher information as the model reaches sufficient capacity to memorize the training set. This is expected behaviour, as the model is likely to become more sensitive to parameter changes when it has memorized the training set. However, we observe the average Fisher information decreasing again as model capacity increases further. This is unexpected, as we would expect the model to remain sensitive to parameter changes after memorizing the training set.

Considering the minimal improvement in test performance, we chose to test increasing the size of the training set to 50 training seeds and the training episodes to 20,000 episodes per model. Additionally, we tested a training set of 100 training maps and 50,000 episodes per training run. Figures \ref{fig:trpo_capacity_50seed} and \ref{fig:trpo_capacity_100seed} show the results of these experiments.

\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/50seed_20k.png}
    \caption{TRPO training and test performance, and average Fisher information, as a function of model capacity, using 50 training seeds and 20,000 training episodes per model.}
    \label{fig:trpo_capacity_50seed}
\end{figure}

\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/100seed_50k.png}
    \caption{TRPO training and test performance, and average Fisher information, as a function of model capacity, using 100 training seeds and 50,000 training episodes per model.}
    \label{fig:trpo_capacity_100seed}
\end{figure}

Here, we can observe similar results to the previous experiment, but with slightly improved test performance after the first ascent to around -0.3 for 50 training maps and 0.0 for 100 training maps. The training performance decreases slightly with larger amounts of training maps, with mean return keeping around 0.8 for 50 training maps and 0.7 for 100 training maps. Again, no second ascent is observed in the test performance, and thus no DD. The average Fisher information again spikes as the model reaches its highest performance on the training set but lags behind in test performance, and again decreases as model capacity increases further.

To test whether the model type was able to learn an unseeded (that is, without a difference in training and test maps) environment, we tested a a configuration with 1,000 training seeds when training for 100,000 episodes per mode. Figure \ref{fig:trpo_capacity_1000seed_short} shows the results of this experiment.

\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/1000seed_100k.png}
    \caption{TRPO training and test performance, and average Fisher information, as a function of model capacity, using 1,000 training seeds and 100,000 training episodes per model.}
    \label{fig:trpo_capacity_1000seed_short}
\end{figure}

Here, we can observe that both training and test performance are very similar, as expected when the training set is so large---we completely cover the distribution of possible maps. Performance increases slowly with model capacity, stabilizing at approximately 0 mean return for both training and test performance. Considering no generalization gap emerges at all, we have no DD. The average Fisher information remains low, with one spike near the lowest model capacity, which is caused by an outlier in the data. This aligns with our expectation that the model does not memorize the training set, as it has effectively seen the full distribution of maps already.

At this stage, we noticed the training performance was not reaching the maximum possible return of approximately 0.96 mean return, even at very high model capacities. We hypothesized that increasing the amount of training time per model would allow the models to reach higher training performance, and thus potentially observe DD in test performance. We thus increased the amount of training episodes per model to 1,000,000. For this newly increased training time, we tested training set sizes of 500, 750, and 1,000 maps. Figures \ref{fig:trpo_capacity_500seed}, \ref{fig:trpo_capacity_750seed}, and \ref{fig:trpo_capacity_1000seed} show the results of these experiments.

\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/500seed_1M.png}
    \caption{TRPO training and test performance, and average Fisher information, as a function of model capacity, using 500 training seeds and 1,000,000 training episodes per model.}
    \label{fig:trpo_capacity_500seed}
\end{figure}

\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/750seed_1M.png}
    \caption{TRPO training and test performance, and average Fisher information, as a function of model capacity, using 750 training seeds and 1,000,000 training episodes per model.}
    \label{fig:trpo_capacity_750seed}
\end{figure}

\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/1000seed_1M.png}
    \caption{TRPO training and test performance, and average Fisher information, as a function of model capacity, using 1,000 training seeds and 1,000,000 training episodes per model.}
    \label{fig:trpo_capacity_1000seed}
\end{figure}

This longer training time shows the models reaching much closer to the optimal performance, stabilizing around 0.8 average return on training maps. Unfortunately, even with this increased training time, we were again unable to observe DD in test performance. We see the generalization gap closing slowly with increased training set size, which is expected as the model sees more of the distribution of maps. The average Fisher information continues to spike as the model reaches its highest performance on the training set but lags behind in test performance, and again decreases as model capacity increases further.

Finally, on our initial configurations of 50 training seeds and 20,000 training episodes per model as well as 100 training seeds and 50,000 training episodes per model, we tested the randomized environment configuration, with both agent starting and goal position being selected randomly from the four corners of the map. This way the agent would be unable to use the basic rule to move down and to the right. Figures \ref{fig:50seed_20k_random} and \ref{fig:100seed_50k_random} show the results of this experiment.

\begin{figure}[!hbtp] % again, in general DO NOT include multiple figures that all tell the same story. make plots next to each other not above, escpae column layout
    \centering
    \includegraphics[width=0.45\textwidth]{../images/50seed_20k_random.png}
    \caption{TRPO training and test performance, and average Fisher information, as a function of model capacity, using 50 training seeds and 20,000 training episodes per model and randomized start and goal positions.}
    \label{fig:50seed_20k_random}
\end{figure}

\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/100seed_50k_random.png}
    \caption{TRPO training and test performance, and average Fisher information, as a function of model capacity, using 100 training seeds and 50,000 training episodes per model and randomized start and goal positions.}
    \label{fig:100seed_50k_random}
\end{figure}

Here, we observe different dynamics. The training performance varies strongly with model capacity. After investigating this we can note that this is related to the width and depth of the model. In general, the model performs better with increasing width, but performance decreases upon adding more layers to a specific depth. This may be related to optimization difficulties in deeper networks as they can be more difficult to train, as shown by \cite{he2015deepresiduallearningimage}. It is interesting that this phenomenon is especially strong when the start and goal positions are randomized instead of fixed. The test performance remains almost minimal throughout all model capacities, with a very slight upward trend with model capacity. This indicates that we would need to train on a larger set of training maps to see any generalization capacity whatsoever. The average Fisher information spikes erratically around where the model reaches its highest performance on the training set, but then comes back down again.

\subsection{Episodic Regime}\label{sec:episodic_regime}
As discussed, DD can also occur with respect to the amount of optimization steps taken during training, which we refer to as the episodic regime. To test whether this regime exhibits DD under our conditions, we trained models of depth 3 and widths 4, 16, 64, 256, and 1024 on two configurations: 50 training maps and 100 training maps. Figures \ref{fig:50seed_unlim_1024} and \ref{fig:100seed_unlim_1024} show examples of the learning behavior during training. We trained without a limit on the number of training episodes, but in practice were limited to about 1.6M to 1.8M episodes until our high performance cluster ended the jobs. The remaining configurations exhibited similar behavior, except for the smallest models of width 4, which failed to learn the environment at all. These plots can be found in Appendix \ref{sec:longer-training-runs}.

\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/50seed_unlim_1024.png}
    \caption{TRPO training and test performance over time for a model of width 1024 and depth 3, using 50 training seeds and unlimited training episodes.}
    \label{fig:50seed_unlim_1024}
\end{figure}

\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=0.45\textwidth]{../images/100seed_unlim_1024.png}
    \caption{TRPO training and test performance over time for a model of width 1024 and depth 3, using 100 training seeds and unlimited training episodes.}
    \label{fig:100seed_unlim_1024}
\end{figure}

Again, the results do not exhibit signs of DD. For both training sets, we can observe a quick increase in training performance at the beginning, barely visible due to the scale of the graph. After this initial increase, training performance stays at a near-optimal level. The difference to the optimal level indicates that the model fails to solve a small number of maps or learned a suboptimal policy on maps it can solve. Test performance also increases quickly at the beginning and stabilizes at a low level around -0.2 for 50 training maps and 0 for 100 training maps. However, we can observe a very slight downward trend in test performance over time, indicating that the model may be overfitting slightly to the training set. However, no second ascent is observed in the test performance within our training time, and thus no DD phenomenon. % TODO: rephrase last sentence, clarify DD vs DA