\section{Results}\label{sec:results}
In this section, we present the results of our experiments searching for DD in RL. We were unable to observe DD across any tested configurations in both the episodic and capacity regimes.

\subsection{DQN Fails to Learn}
First, we examined generalization capacity using DQNs on our environment. This architecture was unable to solve the environment altogether. We observe frequent catastrophic forgetting, with increases in mean return being present but short-lived. Examples of a DQN failing to learn the environment reliably can be found in Appendix~\ref{sec:DQN-fails}, along with plots showing DQN mean return as a function of model capacity. These show that the failure to learn occurs over all tested model capacities. No DD is observed, as neither the training return nor the test return exhibit any significant learning. Similarly, the average Fisher information does not exhibit significant change and stays extremely close to zero across our tested model capacities. These results are uninformative for studying DD, as the model fails to learn the environment. Without a rise in train return, a subsequent rise in test return cannot be assessed. Given the poor performance of DQNs on this environment, we turned to an alternative architecture that we expected to perform more effectively.

\subsection{TRPO: No DD}
We next examined generalization capacity using TRPO. This architecture was able to learn the environment reliably across a range of model capacities. However, we were again unable to observe DD in either the episodic or capacity regimes.

\subsubsection{Capacity Regime}
\begin{figure*}[!hbtp]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/10seed_10k.png}
        \caption{Trained on 10 maps for 10,000 episodes.}
        \label{fig:trpo_capacity_10seed}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/500seed_1M.png}
        \caption{Trained on 500 maps for 1,000,000 episodes.}
        \label{fig:trpo_capacity_500seed}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/50seed_randcorner_20k.png}
        \caption{Trained on 50 maps for 20,000 episodes with randomized start and goal positions.}
        \label{fig:50seed_20k_random}
    \end{minipage}\hfill
\end{figure*}
In the capacity regime, we conducted studies to heuristically search the available hyperparameter space. Our two hyperparameters to calibrate here are size of the train set (in seeds) and amount of training time (in episodes). We varied both hyperparameters across a range of values, as shown in Table~\ref{table:runs_capacity}. To plot the mean returns, we normalize by using the mean minimal and optimal returns across the seeds used for the specific configuration to scale the returns between 0 and 1, enabling comparisons between different environment configurations.
First, we used 10 train maps and 10,000 training episodes per model, with the standard environment configuration. We can observe that both training and test returns are both low for the smallest model capacities. As we increase model capacity, training return increases rapidly until it reaches the mean maximum return. Test return increases slightly but stabilizes quickly at about 10\% of mean maximum return. No second rise is observed in the test return when increasing model capacity, and thus we cannot observe DD. At the same time, we observe a spike in the average Fisher information as the gap in mean return on the train and test set emerges, which subsides again as model capacity increases further. Figure~\ref{fig:trpo_capacity_10seed} shows the results of this experiment. Considering the minimal improvement in test return, we increased the train set size as well as the training episodes. We tested a train set size of 50 seeds and 20,000 training episodes, a train set size of 100 train maps and 50,000 episodes, and a train set size of 1,000 maps and 100,000 episodes. Here, we can observe similar results, with slightly improved test return after the first rise to about 0.2 of mean maximum return for 50 train maps and about 0.4 for 100 train maps. The training return decreases with larger amounts of train maps, with mean return keeping around 0.8 for 50 train maps and 0.7 for 100 train maps. In these two configurations, no DD can be observed as the generalization gap does not close when increasing model capacity. As before, the average Fisher information again spikes as the train and test return diverges, but again decreases as model capacity increases further. When using 1,000 train maps, the train and test remain approximately equal and no generalization gap emerges at all, with both train and test return stabilizing at about 0.4 of mean maximum return.
Considering there is no generalization gap, we cannot observe DD. In this configuration we also cannot observe the spike in the average Fisher information that emerges with the generation gap. There is one high outlier value, but no sustained increase in the average Fisher information as in the other configurations. Plots showing these results can be found in Appendix~\ref{sec:appendix_trpo}.

At this stage, there is no generalization gap, and thus further studying such configurations is not useful to our study of DD. To continue with large train sets, we need to allow the model to learn longer and reach higher training return. We thus increased the amount of training episodes per model to 1,000,000. For this increased training time, we tested train set sizes of 100, 500, 750, and 1,000 maps. This longer training time shows the models reaching much closer to the maximum return. However, even with this increased training time, we were again unable to observe DD in test return. We see the generalization gap closing slowly with increased train set size, which is expected as the model sees more of the distribution of all possible maps and consistent with our results using fewer training episodes. The average Fisher information continues to spike as the model reaches its highest return on the train set but lags behind in test return, and decreases as model capacity increases further. An example of these results is shown in Figure~\ref{fig:trpo_capacity_500seed}, with other configurations showing similar results in Appendix~\ref{sec:appendix_trpo}.

Finally, on the configurations of 50 train maps and 20,000 training episodes per model as well as 100 train maps and 50,000 training episodes per model, we tested the randomized environment configuration, with both agent starting and goal position being selected randomly from the four corners of the map. This way the agent would be unable to use the basic rule of down and to the right to reach the goal. Here, we observe that mean return varies strongly with model capacity, specifically with the depth of the model, with deeper models performing worse than shallower models when keeping the width fixed. This may be related to optimization difficulties in deeper networks as they can be more difficult to train, as shown by \cite{he2015deepresiduallearningimage}. % todo: say something about how it is interesting this emerges especially with randomized configuration? % todo: maybe this is not the best paper to cite for this?

The test return remains very low throughout all model capacities. This indicates that we would need to train on a larger set of train maps to see any generalization capacity whatsoever. The average Fisher information spikes approximately when the model reaches its highest return on the train set, but then comes back down again. This is similar to our earlier findings of the average Fisher information spiking as the generalization gap emerges. The results are visualized in Figure~\ref{fig:50seed_20k_random} and in Appendix~\ref{sec:appendix_trpo}.

The initial increase of the average Fisher information is expected behaviour, as the model is likely to become more sensitive to parameter changes when it is overfitting on the train set. However, we consistently observe the average Fisher information decreasing again as model capacity increases further. This is unexpected, as we would expect the model to remain sensitive to parameter changes after memorizing the train set. One reason this could be occurring is because we normalize by dividing by parameter count of the model. We observe that the average Fisher information is highest at the capacity where the model starts to overfit on the train set, but as we increase capacity further, the added parameters may add dimensions of low or zero Fisher information, which would lower the average even if the total Fisher information without normalization remains high. However, after examining the absolute FIM trace, we see a very similar trend. This suggests that the decrease in average Fisher information could be strengthened by the normalization, but is not solely caused by it. A selection of plots including both average Fisher information and absolute FIM trace can be found in Appendix~\ref{sec:appendix_absolute_fim}. Another possible cause is that the Fisher information we estimate is on-policy and therefore tightly coupled to the state--action visitation distribution induced by the current policy. Concretely, the empirical FIM for a policy $\pi_\theta(a \mid s)$ is an expectation of score outer products, as defined in Equation \eqref{eq:fim_definition}. This means that a decrease in the average Fisher information can reflect a change in how stochastic the policy is, rather than an improvement in out-of-distribution generalization. Because the environment is Markovian and fully observable, there exists an optimal deterministic stationary policy (up to tie-breaking). Therefore, as we increase the capacity of the models, they may be able to learn a closer to optimal, and thus more deterministic policy on the train set, which would thus have lower Fisher information. This can coexist with poor returns on the test maps, as the policy can be locally stable and low-sensitivity on the training-induced state distribution while still failing catastrophically on unseen layouts that induce different state visitation. Additionally, if this is the case, the distribution of states visited by the model would also shrink as the capacity grows and the model becomes explores less and exploits more. If fewer states are regularly visited, then the average can decrease even if the policy remains sensitive to parameter changes on those states.

% we can measure this using the effective rank!
% todo: mention that we checked whether this is caused by the normalization by parameter count, but the same trend is present even without normalization.
If these hypotheses are accurate, this questions the usefulness of the average Fisher information as a measure of generalization capacity in RL, as it may be confounded by changes in the state--action visitation distribution and policy stochasticity rather than reflecting true improvements in out-of-distribution generalization.

% Future work could investigate this further by measuring the effective rank of the FIM, which would give insight into how many dimensions of high sensitivity the model has, and whether this changes with capacity. It would also be interesting to examine the state visitation distribution across training runs and model capacities to see if it shrinks as capacity increases, which would support the hypothesis that the model is becoming more deterministic and thus visiting fewer states.

\subsection{Episodic Regime}\label{sec:episodic_regime}
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=\linewidth]{../images/100s_w1024.png}
    \caption{TRPO training and test return over time for a model of width 1024 and depth 3, using 100 train maps and unlimited training episodes.}
    \label{fig:100seed_unlim_1024}
\end{figure}
DD can also occur with respect to the amount of optimization steps taken during training, which we refer to as the episodic regime. To test whether this regime exhibits DD under our conditions, we trained models of depth 3 and widths 4, 16, 64, 256, and 1024 on two configurations: 50 train maps and 100 train maps. Figure~\ref{fig:100seed_unlim_1024} shows an example of the learning behavior during training. We trained without a limit on the number of training episodes, but in practice were limited to about 1.6M to 1.8M episodes until our high performance cluster ended the jobs. The remaining configurations exhibited similar behavior, except for the smallest models of width 4, which failed to learn the environment at all. These plots can be found in Appendix~\ref{sec:longer-training-runs}.

Again, the results do not exhibit signs of DD. We can observe a quick increase in train return at the beginning, barely visible due to the scale of the graph. After this initial increase, training return stays at a near-optimal level. The difference to the optimal level indicates that the model fails to solve a small number of maps or learned a suboptimal policy on maps it can solve. Test return also increases quickly at the beginning and stabilizes at a low level between 0.2 and 0.4 for the tested configurations. However, we can observe a very slight downward trend in test return over time, indicating that the model may be overfitting slightly to the train set. However, no second rise is observed in the test return within our training time, and thus no DD phenomenon. % TODO: rephrase last sentence, clarify DD vs DA