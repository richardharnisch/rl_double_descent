\section{Results}\label{sec:results}
In this section, we present the results of our experiments searching for DD in RL. Unfortunately, across all configurations tested, we did not observe instances of DD in either the episodic or capacity regime.

\subsection{DQN Fails to Learn}
First, we examined generalization capacity using DQNs on our environment. Unfortunately, this architecture was unable to solve the environment altogether. We observe frequent catastrophic forgetting, with increases in mean return being present but short-lived. Examples of a DQN failing to learn the environment reliably can be found in Appendix \ref{sec:DQN-fails}, along with plots showing DQN mean return as a function of model capacity. These show that the failure to learn occurs over all tested model capacities. No DD is observed, as neither the training return nor the test return exhibit any significant learning. Similarly, the average Fisher information does not exhibit significant change and stays extremely close to zero across our tested model capacities. These results are uninformative for studying DD, as the model fails to learn the environment. Without a rise in train return, a subsequent ascent in test return cannot be assessed. Given the poor performance of DQNs on this environment, we therefore turned to an alternative architecture that we expected to perform more effectively.

\subsection{TRPO: No DD}
We next examined generalization capacity using TRPO. This architecture was able to learn the environment reliably across a range of model capacities. However, we were again unable to observe DD in either the episodic or capacity regimes.

\subsubsection{Capacity Regime}
\begin{figure*}[!hbtp]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/10seed_10k.png}
        \caption{Trained on 10 maps for 10,000 episodes.}
        \label{fig:trpo_capacity_10seed}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/500seed_1M.png}
        \caption{Trained on 500 maps for 1,000,000 episodes.}
        \label{fig:trpo_capacity_500seed}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/50seed_randcorner_20k.png}
        \caption{Trained on 50 maps for 20,000 episodes with randomized start and goal positions.}
        \label{fig:50seed_20k_random}
    \end{minipage}\hfill
\end{figure*}
In the capacity regime, we conducted studies to heuristically search the available hyperparameter space. Our two hyperparameters to calibrate here are size of the training set (in seeds) and amount of training time (in episodes). We varied both hyperparameters across a range of values, as shown in Table \ref{table:runs_capacity}. To plot the mean returns, we normalize by using the mean minimal and optimal returns across the seeds used for the specific configuration to scale the returns between 0 and 1. This makes the results more interpretable between different environment configurations.
First, we used 10 train maps and 10,000 training episodes per model, with the standard environment configuration. We can observe that both training and test returns are both low for the smallest model capacities. As we increase model capacity, training return increases rapidly until it reaches the mean maximum return. Test return increases slightly but stabilizes quickly at about 10\% of mean maximum return. No second ascent is observed in the test return when increasing model capacity, and thus we cannot observe DD. At the same time, we observe a spike in the average Fisher information as the gap in mean return on the train and test set emerges, which subsides again as model capacity increases further. Figure \ref{fig:trpo_capacity_10seed} shows the results of this experiment. Considering the minimal improvement in test return, we increased the training set size as well as the training episodes. We tested a training set size of 50 seeds and 20,000 training episodes, a training set size of 100 train maps and 50,000 episodes, and a training set size of 1,000 maps and 100,000 episodes. Here, we can observe similar results, with slightly improved test return after the first ascent to about 0.2 of mean maximum return for 50 train maps and about 0.4 for 100 train maps. The training return decreases with larger amounts of train maps, with mean return keeping around 0.8 for 50 train maps and 0.7 for 100 train maps. In these two configurations, no DD can be observed as the generalization gap does not close when increasing model capacity. As before, the average Fisher information again spikes as the train and test return diverges, but again decreases as model capacity increases further. When using 1,000 train maps, the train and test remain approximately equal and no generalization gap emerges at all, with both train and test return stabilizing at about 0.4 of mean maximum return.
Considering there is no generalization gap, we cannot observe DD. In this configuration we also cannot observe the spike in the average Fisher information that emerges with the generation gap. There is one high outlier value, but no sustained increase in the average Fisher information as in the other configurations. Plots showing these results can be found in Appendix \ref{sec:appendix_trpo}.

At this stage, there is no generalization gap, and thus further studying such configurations is not useful to our study of DD. To continue with large train sets, we need to allow the model to learn longer and reach higher training return. We thus increased the amount of training episodes per model to 1,000,000. For this increased training time, we tested train set sizes of 100, 500, 750, and 1,000 maps. This longer training time shows the models reaching much closer to the maximum return. However, even with this increased training time, we were again unable to observe DD in test return. We see the generalization gap closing slowly with increased training set size, which is expected as the model sees more of the distribution of all possible maps and consistent with our results using fewer training episodes. The average Fisher information continues to spike as the model reaches its highest return on the training set but lags behind in test return, and decreases as model capacity increases further. An example of these results is shown in Figure \ref{fig:trpo_capacity_500seed}, with other configurations showing similar results in Appendix \ref{sec:appendix_trpo}.

Finally, on the configurations of 50 train maps and 20,000 training episodes per model as well as 100 train maps and 50,000 training episodes per model, we tested the randomized environment configuration, with both agent starting and goal position being selected randomly from the four corners of the map. This way the agent would be unable to use the basic rule of down and to the right to reach the goal. Here, we observe that mean return varies strongly with model capacity, specifically with the depth of the model, with deeper models performing worse than shallower models when keeping the width fixed. This may be related to optimization difficulties in deeper networks as they can be more difficult to train, as shown by \cite{he2015deepresiduallearningimage}. % todo: say something about how it is interesting this emerges especially with randomized configuration? % todo: maybe this is not the best paper to cite for this?
The test return remains very low throughout all model capacities. This indicates that we would need to train on a larger set of train maps to see any generalization capacity whatsoever. The average Fisher information spikes approximately when the model reaches its highest return on the training set, but then comes back down again. This is similar to our earlier findings of the average Fisher information spiking as the generalization gap emerges. The results are visualized in Figure \ref{fig:50seed_20k_random} and in Appendix \ref{sec:appendix_trpo}.

The initial increase of the average Fisher information is expected behaviour, as the model is likely to become more sensitive to parameter changes when it is overfitting on the train set. However, we consistently observe the average Fisher information decreasing again as model capacity increases further. This is unexpected, as we would expect the model to remain sensitive to parameter changes after memorizing the training set. One possible reason this is occuring is because we use a softmax function for the policy, which leads to increasing policy determinisim. Considering this environment is Markovian, the optimal policy is deterministic. As the model reaches higher return on the training set, it may be learning a more deterministic policy, which is less sensitive to parameter changes. This could explain why the average Fisher information decreases again after spiking as the model reaches high returns on the training set.
% TODO: go into more detail about how this works

Another reason this could be occurring is because we normalize by dividing by parameter count of the model. We observe that the average Fisher information is highest at the capacity where the model starts to

TODO: what is your hypothesis on why this is the case? Why does this happen?

\subsection{Episodic Regime}\label{sec:episodic_regime}
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=\linewidth]{../images/100s_w1024.png}
    \caption{TRPO training and test return over time for a model of width 1024 and depth 3, using 100 train maps and unlimited training episodes.}
    \label{fig:100seed_unlim_1024}
\end{figure}
DD can also occur with respect to the amount of optimization steps taken during training, which we refer to as the episodic regime. To test whether this regime exhibits DD under our conditions, we trained models of depth 3 and widths 4, 16, 64, 256, and 1024 on two configurations: 50 train maps and 100 train maps. Figure \ref{fig:100seed_unlim_1024} shows an example of the learning behavior during training. We trained without a limit on the number of training episodes, but in practice were limited to about 1.6M to 1.8M episodes until our high performance cluster ended the jobs. The remaining configurations exhibited similar behavior, except for the smallest models of width 4, which failed to learn the environment at all. These plots can be found in Appendix \ref{sec:longer-training-runs}.

Again, the results do not exhibit signs of DD. We can observe a quick increase in train return at the beginning, barely visible due to the scale of the graph. After this initial increase, training return stays at a near-optimal level. The difference to the optimal level indicates that the model fails to solve a small number of maps or learned a suboptimal policy on maps it can solve. Test return also increases quickly at the beginning and stabilizes at a low level between 0.2 and 0.4 for the tested configurations. However, we can observe a very slight downward trend in test return over time, indicating that the model may be overfitting slightly to the training set. However, no second ascent is observed in the test return within our training time, and thus no DD phenomenon. % TODO: rephrase last sentence, clarify DD vs DA