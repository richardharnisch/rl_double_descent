\section{Results}\label{sec:results}
In this section, we present the results of our experiments searching for DD in RL. We were unable to observe DD across any tested configurations in both the episodic and capacity regimes.

\subsection{DQN Fails to Learn}
First, we examined generalization capacity using DQNs on our environment. This architecture was unable to solve the environment altogether. We observe frequent catastrophic forgetting, with increases in mean return being present but short-lived. Examples of a DQN failing to learn the environment reliably can be found in Appendix~\ref{sec:DQN-fails}, along with plots showing DQN mean return as a function of model capacity. These show that all tested model capacities fail to learn a meaningful policy. Similarly, the average Fisher information does not exhibit significant change and remains extremely close to zero across our tested model capacities. Given the poor performance of DQNs on this environment, the results are uninformative to our study of DD and we turned to an alternative architecture that we expected to perform more effectively.

\subsection{TRPO: No DD}
We next examined generalization capacity using TRPO. This architecture was able to learn the environment reliably across a range of model capacities. However, we are unable to observe DD in either the episodic or capacity regimes.

\subsubsection{Capacity Regime}
\begin{figure*}[!hbtp]
    \centering
    \hspace*{\fill}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/10seed_10k.png}
        \caption{Trained on 10 maps for 10,000 episodes.}
        \label{fig:trpo_capacity_10seed}
    \end{minipage}
    \hspace*{\fill}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../images/500seed_1M.png}
        \caption{Trained on 500 maps for 1,000,000 episodes.}
        \label{fig:trpo_capacity_500seed}
    \end{minipage}
    \hspace*{\fill}
\end{figure*}
In the capacity regime, we conducted studies to heuristically search the available hyperparameter space. Our two hyperparameters to calibrate are the size of the train set (in seeds) and the amount of training steps (in episodes). To plot the mean returns, we normalize by using the mean minimum and maximum returns across the seeds used for the configuration to scale the returns between 0 and 1, enabling comparisons between different environment configurations. First, we used 10 train maps and 10,000 training episodes per model, with the standard environment configuration. We can observe that training and test returns are both low for the smallest model capacities. As we increase model capacity, training return increases rapidly until it reaches the mean maximum return. Test return increases slightly but stabilizes quickly at about 0.1 of mean maximum return. No second rise is observed in the test return when increasing model capacity, and thus we do not observe DD. At the same time, we observe a spike in the average Fisher information as the gap in mean return on the train and test set emerges, which subsides again as model capacity increases further. Figure~\ref{fig:trpo_capacity_10seed} shows the results of this experiment. Considering the minimal improvement in test return, we increased the train set size as well as the training episodes. We tested a train set size of 50 seeds and 20,000 training episodes, a train set size of 100 train maps and 50,000 episodes, and a train set size of 1,000 maps and 100,000 episodes. Here, we can observe similar results, with slightly improved test return after the first rise to about 0.2 of mean maximum return for 50 train maps and about 0.4 for 100 train maps. The training return decreases with larger amounts of train maps, with mean return keeping around 0.8 for 50 train maps and 0.7 for 100 train maps. In these two configurations, no DD can be observed as the generalization gap does not close when increasing model capacity. As before, the average Fisher information spikes as the train and test return diverges, but decreases when model capacity increases further. Using 1,000 train maps, the train and test remain approximately equal and no generalization gap emerges at all, with both train and test return stabilizing at about 0.4 of mean maximum return. Considering there is no generalization gap, we do not observe DD. In this configuration we also do not observe the spike in the average Fisher information that emerges with the generation gap. There is one high outlier value, but no sustained increase in the average Fisher information as in the other configurations. Plots showing these results can be found in Appendix~\ref{sec:appendix_trpo}.

At this stage, there is no generalization gap, and thus further studying these configurations is not useful to our study of DD. To continue with large train sets, we need to allow the model to reach higher train return. We thus increased the amount of training episodes per model to 1,000,000. For this increased training time, we tested train set sizes of 100, 500, 750, and 1,000 maps. Here, the models reach much closer to the maximum return. However, even with this increased training time, we remain unable to observe DD. We see the generalization gap closing as train set size increases, which we attribute to an implicit curriculum effect and is consistent with our results using fewer training episodes. The average Fisher information continues to spike as the generalization gap between train and test return emerges and decreases as model capacity increases further. An example of these results is shown in Figure~\ref{fig:trpo_capacity_500seed}, with other configurations showing similar results in Appendix~\ref{sec:appendix_trpo}.

Finally, on the configurations of 50 train maps and 20,000 training episodes per model as well as 100 train maps and 50,000 training episodes per model, we tested the randomized environment configuration, where the agent is unable to use the basic rule of moving down and to the right to reach the goal. Here, we observe that mean return varies strongly with model capacity, specifically with the depth of the model, with deeper models performing worse than shallower models when keeping the width fixed. This may be related to optimization difficulties in deeper networks as they can be more difficult to train \citep{glorot2010understandingdifficultytrainingdeepfeedforward}. The test return remains very low throughout all model capacities. This indicates that we would need to train on a larger set of train maps to see any generalization capacity whatsoever. The average Fisher information spikes approximately when the model reaches its highest return on the train set, but then comes back down again. This is similar to our earlier findings of the average Fisher information spiking as the generalization gap emerges. The results are visualized in Appendix~\ref{sec:appendix_trpo}.

The initial increase of the average Fisher information is expected behaviour, as the model is likely to become more sensitive to parameter changes when it is overfitting on the train set. However, we consistently observe the average Fisher information decreasing again as model capacity increases further. This is unexpected, as we would expect the model to remain sensitive to parameter changes after memorizing the train set. One cause could be that we normalize by dividing by parameter count of the model. We observe that the average Fisher information is highest at the capacity where the model starts to overfit on the train set, but as we increase capacity further, the added parameters may add dimensions of low or zero Fisher information, which would lower the average even if the total Fisher information without normalization remains high. However, after examining the absolute FIM trace, we see a very similar trend. This suggests that the decrease in average Fisher information could be strengthened by the normalization, but is not solely caused by it. A selection of plots including both average Fisher information and absolute FIM trace can be found in Appendix~\ref{sec:appendix_absolute_fim}. Another possible cause is that the Fisher information we estimate is on-policy and therefore connected to the state--action visitation distribution induced by the current policy. Concretely, because the empirical FIM for a policy $\pi_\theta(a \mid s)$ is an expectation of score outer products as defined in Equation \eqref{eq:fim_definition}, a decrease in the average Fisher information can reflect a change in how stochastic the policy is, rather than an improvement in out-of-distribution generalization. Because the environment is Markovian and fully observable, there exists an optimal deterministic stationary policy (up to tie breaking). Therefore, as we increase the capacity of the models, they may be able to learn a closer to optimal and thus more deterministic policy on the train set, which would have lower average Fisher information. This can coexist with poor returns on the test maps, as the policy can be locally stable and low-sensitivity on the training-induced state distribution while still failing catastrophically on unseen layouts that induce different state visitation. Additionally, if this is the case, the distribution of states visited by the model would also shrink as the capacity grows and the model explores less and exploits more. If fewer states are regularly visited, then the average can decrease even if the policy remains sensitive to parameter changes on those states.
If these hypotheses are accurate, this questions the usefulness of the average Fisher information as a measure of generalization capacity in RL, as it may be confounded by changes in the state--action visitation distribution and policy stochasticity rather than reflecting true improvements in out-of-distribution generalization.

\subsection{Episodic Regime}\label{sec:episodic_regime}
\begin{figure}[!hbtp]
    \centering
    \includegraphics[width=\linewidth]{../images/100s_w1024.png}
    \caption{TRPO training and test return over time for a model of width 1024 and depth 3, using 100 train maps and unlimited training episodes.}
    \label{fig:100seed_unlim_1024}
\end{figure}
DD can also occur in episodic regime. To test whether this regime exhibits DD under our conditions, we trained models of depth 3 and widths 4, 16, 64, 256, and 1024 on two configurations: 50 train maps and 100 train maps. Figure~\ref{fig:100seed_unlim_1024} shows an example of the learning behavior during training. We trained without a limit on the number of training episodes, but in practice were limited depending on model size to 1.6M--5M episodes by our high-performance computing cluster. The remaining configurations exhibited similar behavior, except for the smallest models of width 4, which failed to learn the environment at all. These plots can be found in Appendix~\ref{sec:longer-training-runs}. Again, the results do not exhibit signs of DD. We can observe a rapid increase in train return at the beginning, barely visible due to the scale of the plot. After this initial increase, training return stays at a near-maximum level. The difference to the maximum level indicates that the model fails to solve a small number of maps or learned a suboptimal policy on maps it can solve. Test return also increases quickly at the beginning and stabilizes at a low level between 0.2 and 0.4 for the tested configurations. However, we observe a slight downward trend in test return over time, indicating that the model may be overfitting slightly to the train set. Nevertheless, no second rise in test return occurs within our training time, and thus we are unable to observe DD.