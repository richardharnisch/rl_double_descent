\section{Training Hyperparameters}\label{sec:appendix_hyperparameters}
\subsection{DQN}
We use a replay buffer size of 50,000 transitions and a target network update frequency of 5,000 training steps. The DQN is trained using the Adam optimizer \citep{kingma2017adammethodstochasticoptimization} with a learning rate of $10^{-3}$ and a batch size of 64. The exploration-exploitation trade-off is managed using an epsilon-greedy strategy, where the exploration rate $\epsilon$ decays linearly from 1.0 to 0.05 over the first 30\% of the episodes.
\subsection{TRPO}
The TRPO hyperparameters are a maximum KL divergence of $\delta=10^{-2}$, conjugate-gradient iterations set to 10 with damping 0.1, backtracking coefficient 0.5 for 10 line-search steps, GAE $\lambda=0.95$, and a batch size of 20 episodes per policy update. The value function is optimized for 5 iterations per update using Adam with learning rate $10^{-3}$.
