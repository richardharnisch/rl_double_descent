\section{Introduction}\label{sec:introduction}
Double Descent is a well-studied phenomenon in supervised learning. When increasing model capacity, training time, or dataset size, the test risk of deep models begins to improve (decrease) with the train risk up until a "sweet spot" at which test risk is traditionally understood to be minimized. Past this point, increasing model capacity, training time, or dataset size leads to overfitting, a behavior where the model memorizes the training data and thus fails to generalize over unseen test data. In other words, a generalization gap grows between the test performance and the train performance. This behavior can be visualized as a U-shaped risk curve, as shown in Figure \ref{fig:u_shaped}.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{../images/u-shaped.pdf}
    \caption{Traditional U-shaped risk curve. The training and test risk initially decrease together, until the model begins overfitting. Figure taken from \cite{Belkin_2019}.}
    \label{fig:u_shaped}
\end{figure}

However, work in the past years has shown that this traditional U-shaped risk curve is not the end of the story. Instead, after this initial overfitting phase, increasing model capacity, training time, or dataset size further leads to a second descent in test risk \citep{Belkin_2019,nakkiran2019deepdoubledescentbigger}, as can be seen in Figure \ref{fig:double_descent}. This phenomenon has been coined "Double Descent" (DD) and has been observed across various architectures and datasets in supervised learning \citep{nakkiran2019deepdoubledescentbigger}.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{../images/doubledescent.pdf}
    \caption{Double Descent risk curve. After the initial overfitting phase, increasing model capacity, training time, or dataset size further leads to a second descent in test risk. Figure taken from \cite{Belkin_2019}.}
    \label{fig:double_descent}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{../images/nakkiran_double_descent.png}
    \caption{Double Descent observed in practice in Supervised Learning on CIFAR-10 with ResNets, varying model size (left), and training time (middle); showing Double Descent is possible under both regimes. Figure taken from \cite{nakkiran2019deepdoubledescentbigger}.}
    \label{fig:nakkiran_dd}
\end{figure}

The reason for this behavior remains unsolved, but with a number of complementary hypotheses.

% TODO: explain hypotheses, perhaps using nakkiran 2019

The underlying mechanisms behind DD are still debated, but several complementary hypotheses have emerged in the supervised learning literature. A unifying perspective is that DD becomes most visible near the interpolation threshold: the regime where the model becomes expressive enough to fit the training data almost perfectly. Around this point, small changes in model size or training procedure can cause large changes in generalization, producing the characteristic peak in test risk \citep{Belkin_2019,nakkiran2019deepdoubledescentbigger}.

One commonly cited explanation is that the interpolation threshold creates a variance spike in the learned function. As the model approaches the capacity to fit the training data exactly, it must contort its decision boundary or function representation to accommodate all training points, including noise or outliers. This leads to high variance in predictions on unseen data, harming generalization and producing the peak in test risk \citep{Belkin_2019}. Adding more parameters allows the model to fit the data more smoothly, reducing variance and leading to the second descent. In linear regression, this can be shown explicitly because the variance of the estimator diverges at the interpolation threshold \citep{hastie2020surpriseshighdimensionalridgelesssquares}.

Even when models are large enough to interpolate, training algorithms such as (stochastic) gradient descent do not pick arbitrary interpolating solutions. Instead, optimization exhibits implicit regularization. It tends to select solutions with particular simplicity biases (e.g., small norms or large margins), which can generalize well in overparameterized settings. From this view, the second descent occurs because, with sufficient capacity, the optimizer can find interpolating solutions that also satisfy these implicit biases \citep{Belkin_2019}.

Finally, DD is often most pronounced when there is irreducible noise in the targets (e.g., label noise). In that case, interpolating the training set requires fitting noise as well as signal. Near the interpolation threshold, the model begins to fit this noise, harming generalization and producing the peak. With further overparameterization (and sufficient training), the model can fit noise in a way that is less damaging to the learned representation, yielding the second descent \citep{nakkiran2019deepdoubledescentbigger}.

\subsection{Prior Work on DD in RL}
While DD is well documented in supervised learning, its status in reinforcement learning is less clear and appears to depend strongly on the learning setup and on what is used as a proxy for generalization. \cite{vesel√Ω2025presencedoubledescentdeepreinforcement} investigate DD in deep model-free RL by varying network capacity and analyze DD-like behavior through an information-theoretic lens, using policy entropy as a central diagnostic. They report that DD-style patterns can emerge in certain actor--critic settings, but emphasize that standard RL objectives may be unreliable indicators of generalization and call for evaluation on explicit out-of-distribution benchmarks. They note that further work testing whether their observations translate to superior generalization on out-of-distribution generalization. Complementing this empirical perspective, \cite{brellmann2024doubledescentreinforcementlearning} provide a theoretical account of DD in an RL context by studying policy evaluation with regularized LSTD and random features. In a regime where the number of parameters $N$ and the number of distinct visited states $m$ grow proportionally, they derive limiting expressions for the (empirical and true) mean-squared Bellman error and show that DD-like behavior is governed by the ratio $N/m$, with a peak near the interpolation threshold $N/m \approx 1$, and that stronger $L_2$ regularization and improved state coverage attenuate or remove the effect. Together, these works suggest that DD can arise in RL under specific conditions.