% discusses double descent in RL with inference threshold
@misc{brellmann2024doubledescentreinforcementlearning,
  title         = {On Double Descent in Reinforcement Learning with LSTD and Random Features},
  author        = {David Brellmann and Eloïse Berthier and David Filliat and Goran Frehse},
  year          = {2024},
  eprint        = {2310.05518},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2310.05518}
}

% matthia's paper about dd in rl
@misc{veselý2025presencedoubledescentdeepreinforcement,
  title         = {On The Presence of Double-Descent in Deep Reinforcement Learning},
  author        = {Viktor Veselý and Aleksandar Todorov and Matthia Sabatelli},
  year          = {2025},
  eprint        = {2511.06895},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2511.06895}
}

@article{towers2024gymnasium,
  title   = {Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author  = {Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  journal = {arXiv preprint arXiv:2407.17032},
  year    = {2024}
}

% original DQN paper
@article{Mnih2015,
  author   = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  title    = {Human-level control through deep reinforcement learning},
  journal  = {Nature},
  year     = {2015},
  volume   = {518},
  number   = {7540},
  pages    = {529--533},
  doi      = {10.1038/nature14236},
  url      = {https://doi.org/10.1038/nature14236},
  issn     = {1476-4687},
  abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.}
}

@article{Rocks2022MemorizingWithoutOverfitting,
  title     = {Memorizing without overfitting: Bias, variance, and interpolation in overparameterized models},
  author    = {Rocks, Jason W. and Mehta, Pankaj},
  journal   = {Phys. Rev. Res.},
  volume    = {4},
  issue     = {1},
  pages     = {013201},
  numpages  = {19},
  year      = {2022},
  month     = {Mar},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevResearch.4.013201},
  url       = {https://link.aps.org/doi/10.1103/PhysRevResearch.4.013201}
}

% paper showing DD occurs in large neural networks
@misc{nakkiran2019deepdoubledescentbigger,
  title         = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  author        = {Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
  year          = {2019},
  eprint        = {1912.02292},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1912.02292}
}

% hutchinson's estimator for trace of FIM
@article{hutchinson,
  author  = {Hutchinson, M.F.},
  year    = {1989},
  month   = {01},
  pages   = {1059-1076},
  title   = {A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines},
  volume  = {18},
  journal = {Communication in Statistics- Simulation and Computation},
  doi     = {10.1080/03610919008812866}
}

% original TRPO paper
@article{schulman2017trustregionpolicyoptimization,
  title         = {Trust Region Policy Optimization},
  author        = {John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
  journal       = {Proceedings of the 31st International Conference on Machine Learning},
  year          = {2017},
  eprint        = {1502.05477},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1502.05477}
}

% original DQN paper
@article{mnih2013playingatarideepreinforcement,
  title         = {Playing Atari with Deep Reinforcement Learning},
  author        = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  journal       = {arXiv preprint arXiv:1312.5602},
  year          = {2013},
  eprint        = {1312.5602},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1312.5602}
}

@article{fisher1921,
  issn      = {02643952},
  url       = {http://www.jstor.org/stable/91208},
  author    = {R. A. Fisher},
  journal   = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  pages     = {309--368},
  publisher = {Royal Society},
  title     = {On the Mathematical Foundations of Theoretical Statistics},
  urldate   = {2026-01-26},
  volume    = {222},
  year      = {1922}
}
@article{kullback1951information,
  author    = {S. Kullback and R. A. Leibler},
  title     = {{On Information and Sufficiency}},
  volume    = {22},
  journal   = {The Annals of Mathematical Statistics},
  number    = {1},
  publisher = {Institute of Mathematical Statistics},
  pages     = {79 -- 86},
  year      = {1951},
  doi       = {10.1214/aoms/1177729694},
  url       = {https://doi.org/10.1214/aoms/1177729694}
}

@book{sutton1998reinforcementlearningintroduction,
  added-at  = {2019-07-13T10:11:53.000+0200},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  biburl    = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition   = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords  = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title     = {Reinforcement Learning: An Introduction},
  url       = {http://incompleteideas.net/book/the-book-2nd.html},
  year      = {2018 }
}

@book{hastie01statisticallearning,
  added-at  = {2008-05-16T16:17:42.000+0200},
  address   = {New York, NY, USA},
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl    = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords  = {ml statistics},
  publisher = {Springer New York Inc.},
  series    = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title     = {The Elements of Statistical Learning},
  year      = 2001,
  pages     = {221}
}

@inproceedings{kakade2001,
  author    = {Kakade, Sham M},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {T. Dietterich and S. Becker and Z. Ghahramani},
  pages     = {},
  publisher = {MIT Press},
  title     = {A Natural Policy Gradient},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf},
  volume    = {14},
  year      = {2001}
}

@misc{Petersen2008,
  abstract  = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
  added-at  = {2011-01-17T12:52:58.000+0100},
  author    = {Petersen, K. B. and Pedersen, M. S.},
  biburl    = {https://www.bibsonomy.org/bibtex/263c840382cc4b1efb8cefe447465b7ac/hkayabilisim},
  file      = {:home/hkaya/Projeler/diagnus/Screener/doc/literature/Petersen2008.pdf:PDF},
  interhash = {6368b9b490c0225e22334ea0a0841a33},
  intrahash = {63c840382cc4b1efb8cefe447465b7ac},
  keywords  = {matrixderivative inverse Matrixidentity matrixrelations},
  month     = oct,
  note      = {Version 20081110},
  publisher = {Technical University of Denmark},
  review    = {Matrix Cookbook},
  timestamp = {2011-01-17T12:52:58.000+0100},
  title     = {The Matrix Cookbook},
  url       = {http://www2.imm.dtu.dk/pubdb/p.php?3274},
  year      = 2008
}

@inproceedings{lecun1989handwrittendigitrecognitionbackpropagation,
  author    = {Le Cun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  title     = {Handwritten digit recognition with a back-propagation network},
  year      = {1989},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  abstract  = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1\% error rate and about a 9\% reject rate on zipcode digits provided by the U.S. Postal Service.},
  booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
  pages     = {396–404},
  numpages  = {9},
  series    = {NIPS'89}
}

@misc{kingma2017adammethodstochasticoptimization,
  title         = {Adam: A Method for Stochastic Optimization},
  author        = {Diederik P. Kingma and Jimmy Ba},
  year          = {2017},
  eprint        = {1412.6980},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1412.6980}
}

@misc{cohen2016groupequivariantconvolutionalnetworks,
  title         = {Group Equivariant Convolutional Networks},
  author        = {Taco S. Cohen and Max Welling},
  year          = {2016},
  eprint        = {1602.07576},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1602.07576}
}

@misc{he2015deepresiduallearningimage,
  title         = {Deep Residual Learning for Image Recognition},
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year          = {2015},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1512.03385}
}

@misc{hastie2020surpriseshighdimensionalridgelesssquares,
  title         = {Surprises in High-Dimensional Ridgeless Least Squares Interpolation},
  author        = {Trevor Hastie and Andrea Montanari and Saharon Rosset and Ryan J. Tibshirani},
  year          = {2020},
  eprint        = {1903.08560},
  archiveprefix = {arXiv},
  primaryclass  = {math.ST},
  url           = {https://arxiv.org/abs/1903.08560}
}

@inproceedings{boes1996dynamics,
  author = {Boes, Siegfried and Opper, Manfred},
  year   = {1996},
  month  = {01},
  pages  = {141-147},
  title  = {Dynamics of Training.}
}

@article{deng2012mnist,
  title     = {The mnist database of handwritten digit images for machine learning research},
  author    = {Deng, Li},
  journal   = {IEEE Signal Processing Magazine},
  volume    = {29},
  number    = {6},
  pages     = {141--142},
  year      = {2012},
  publisher = {IEEE}
}

@inproceedings{gunasekar2017implicitregularizationmatrixfactorization,
  author    = {Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  title     = {Implicit regularization in matrix factorization},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix X with gradient descent on a factorization of X. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {6152–6160},
  numpages  = {9},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@inproceedings{li2018algorithmicregularizationoverparameterized,
  title     = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
  author    = {Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle = {Proceedings of the 31st  Conference On Learning Theory},
  pages     = {2--47},
  year      = {2018},
  editor    = {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume    = {75},
  series    = {Proceedings of Machine Learning Research},
  month     = {06--09 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v75/li18a/li18a.pdf},
  url       = {https://proceedings.mlr.press/v75/li18a.html},
  abstract  = {We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. Concretely, we show that given $\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\star}$, we can recover $X^{\star}$ by parameterizing it by $UU^\top$ with $U\in \mathbb R^{d\times d}$ and minimizing the squared loss, even if $r \ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\star}$ in $\tilde{O}(\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.’17 under the restricted isometry property.  The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications.}
}

@article{Spigler2019JammingTransition,
  author  = {Spigler, Stefano and Geiger, Mario and d'Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
  title   = {A jamming transition from under- to over-parametrization affects generalization in deep learning},
  journal = {Journal of Physics A: Mathematical and Theoretical},
  year    = {2019},
  volume  = {52},
  number  = {47},
  pages   = {474001},
  doi     = {10.1088/1751-8121/ab4c8b},
  note    = {Machine Learning and Statistical Physics: Theory, Inspiration, Application}
}

@article{Geiger2020ScalingGeneralization,
  author  = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d'Ascoli, St{\'e}phane and Biroli, Giulio and Hongler, Cl{\'e}ment and Wyart, Matthieu},
  title   = {Scaling description of generalization with number of parameters in deep learning},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  year    = {2020},
  volume  = {2020},
  number  = {2},
  pages   = {023401},
  doi     = {10.1088/1742-5468/ab633c},
  note    = {Interdisciplinary statistical mechanics}
}

@article{Belkin_2019,
  title     = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
  volume    = {116},
  issn      = {1091-6490},
  url       = {http://dx.doi.org/10.1073/pnas.1903070116},
  doi       = {10.1073/pnas.1903070116},
  number    = {32},
  journal   = {Proceedings of the National Academy of Sciences},
  publisher = {Proceedings of the National Academy of Sciences},
  author    = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year      = {2019},
  month     = jul,
  pages     = {15849–15854}
}

@article{song2021generalizationerrorrandomfeaturesregression,
  author  = {Mei, Song and Montanari, Andrea},
  year    = {2021},
  month   = {06},
  pages   = {},
  title   = {The Generalization Error of Random Features Regression: Precise Asymptotics and the Double Descent Curve},
  volume  = {75},
  journal = {Communications on Pure and Applied Mathematics},
  doi     = {10.1002/cpa.22008}
}

@article{Nakkiran_2021,
  doi       = {10.1088/1742-5468/ac3a74},
  url       = {https://doi.org/10.1088/1742-5468/ac3a74},
  year      = {2021},
  month     = {dec},
  publisher = {IOP Publishing and SISSA},
  volume    = {2021},
  number    = {12},
  pages     = {124003},
  author    = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  title     = {Deep double descent: where bigger models and more data hurt*},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  abstract  = {We show that a variety of modern deep learning tasks exhibit a ‘double-descent’ phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.}
}

@misc{nakkiran2019datahurtlinearregression,
  title         = {More Data Can Hurt for Linear Regression: Sample-wise Double Descent},
  author        = {Preetum Nakkiran},
  year          = {2019},
  eprint        = {1912.07242},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1912.07242}
}

@article{loog2020briefprehistorydoubledescent,
  author  = {Marco Loog and Tom Viering and Alexander Mey and Jesse H. Krijthe and David M. J. Tax},
  title   = {A brief prehistory of double descent},
  journal = {Proceedings of the National Academy of Sciences},
  volume  = {117},
  number  = {20},
  pages   = {10625-10626},
  year    = {2020},
  doi     = {10.1073/pnas.2001875117},
  url     = {https://www.pnas.org/doi/abs/10.1073/pnas.2001875117},
  eprint  = {https://www.pnas.org/doi/pdf/10.1073/pnas.2001875117}
}

@article{dAscoli2021TripleDescent,
  author  = {d'Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio},
  title   = {Triple descent and the two kinds of overfitting: where and why do they appear?},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  year    = {2021},
  volume  = {2021},
  number  = {12},
  pages   = {124002},
  doi     = {10.1088/1742-5468/ac3909}
}

@article{Buschjger2021ThereIsNoDoubleDescentRandomForests,
  title   = {There is no Double-Descent in Random Forests},
  author  = {Sebastian Buschj{\"a}ger and Katharina Morik},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2111.04409},
  url     = {https://api.semanticscholar.org/CorpusID:243848097}
}

@inproceedings{curth2023uturndoubledescent,
  author    = {Curth, Alicia and Jeffares, Alan and van der Schaar, Mihaela},
  title     = {A U-turn on double descent: rethinking parameter counting in statistical learning},
  year      = {2023},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a U-shaped curve reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count p grows past sample size n - a phenomenon dubbed double descent. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include linear regression, trees, and boosting. In this work, we take a closer look at the evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to what is being plotted on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple, distinct complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and only) when and where the transition between these underlying axes occurs, and that its location is thus not inherently tied to the interpolation threshold p = n. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as smoothers and propose a generalized measure for the effective number of parameters they use on unseen examples, using which we find that their apparent double descent curves do indeed fold back into more traditional convex shapes - providing a resolution to the ostensible tension between double descent and traditional statistical intuition.},
  booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
  articleno = {2439},
  numpages  = {31},
  location  = {New Orleans, LA, USA},
  series    = {NIPS '23}
}