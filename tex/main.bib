% discusses double descent in RL with inference threshold
@inproceedings{brellmann2024doubledescentreinforcementlearning,
  title     = {On Double Descent in Reinforcement Learning with LSTD and Random Features},
  author    = {David Brellmann and Eloïse Berthier and David Filliat and Goran Frehse},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024},
  url       = {https://openreview.net/forum?id=9RIbNmx984}
}

% matthia's paper about dd in rl
@misc{veselý2025presencedoubledescentdeepreinforcement,
  title         = {On The Presence of Double-Descent in Deep Reinforcement Learning},
  author        = {Viktor Veselý and Aleksandar Todorov and Matthia Sabatelli},
  year          = {2025},
  eprint        = {2511.06895},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2511.06895}
}

@article{towers2024gymnasium,
  title   = {Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author  = {Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  journal = {arXiv preprint arXiv:2407.17032},
  year    = {2024}
}

% original DQN paper
@article{Mnih2015,
  author   = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  title    = {Human-level control through deep reinforcement learning},
  journal  = {Nature},
  year     = {2015},
  volume   = {518},
  number   = {7540},
  pages    = {529--533},
  doi      = {10.1038/nature14236},
  url      = {https://doi.org/10.1038/nature14236},
  issn     = {1476-4687},
  abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.}
}

% paper showing DD occurs in large neural networks
@inproceedings{nakkiran2019deepdoubledescentbigger,
  title     = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  author    = {Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
  booktitle = {The Eighth International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=B1g5sA4twr}
}

% original TRPO paper
@inproceedings{schulman2017trustregionpolicyoptimization,
  title         = {Trust Region Policy Optimization},
  author        = {John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
  booktitle     = {Proceedings of the 32nd International Conference on Machine Learning},
  year          = {2015},
  volume        = {37},
  series        = {Proceedings of Machine Learning Research},
  publisher     = {PMLR},
  eprint        = {1502.05477},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1502.05477}
}

@article{fisher1921,
  issn      = {02643952},
  url       = {http://www.jstor.org/stable/91208},
  author    = {R. A. Fisher},
  journal   = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  pages     = {309--368},
  publisher = {Royal Society},
  title     = {On the Mathematical Foundations of Theoretical Statistics},
  urldate   = {2026-01-26},
  volume    = {222},
  year      = {1922}
}
@article{kullback1951information,
  author    = {S. Kullback and R. A. Leibler},
  title     = {{On Information and Sufficiency}},
  volume    = {22},
  journal   = {The Annals of Mathematical Statistics},
  number    = {1},
  publisher = {Institute of Mathematical Statistics},
  pages     = {79 -- 86},
  year      = {1951},
  doi       = {10.1214/aoms/1177729694},
  url       = {https://doi.org/10.1214/aoms/1177729694}
}

@inproceedings{ng1999policyinvarianceunderrewardtransformations,
  author    = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
  title     = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
  year      = {1999},
  isbn      = {1558606122},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address   = {San Francisco, CA, USA},
  booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
  pages     = {278–287},
  numpages  = {10},
  series    = {ICML '99}
}

@book{sutton2018reinforcementlearningintroduction,
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  edition   = {Second},
  publisher = {The MIT Press},
  title     = {Reinforcement Learning: An Introduction},
  url       = {http://incompleteideas.net/book/the-book-2nd.html},
  year      = {2018}
}

@book{hastie01statisticallearning,
  address   = {New York, NY, USA},
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  publisher = {Springer New York Inc.},
  series    = {Springer Series in Statistics},
  title     = {The Elements of Statistical Learning},
  year      = {2001}
}

@inproceedings{kakade2001,
  author    = {Kakade, Sham M},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {T. Dietterich and S. Becker and Z. Ghahramani},
  pages     = {},
  publisher = {MIT Press},
  title     = {A Natural Policy Gradient},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf},
  volume    = {14},
  year      = {2001}
}

@misc{Petersen2008,
  author    = {Petersen, K. B. and Pedersen, M. S.},
  month     = oct,
  note      = {Version 20081110},
  publisher = {Technical University of Denmark},
  title     = {The Matrix Cookbook},
  url       = {http://www2.imm.dtu.dk/pubdb/p.php?3274},
  year      = {2008}
}

@inproceedings{lecun1989handwrittendigitrecognitionbackpropagation,
  author    = {Le Cun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  title     = {Handwritten digit recognition with a back-propagation network},
  year      = {1989},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  abstract  = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1\% error rate and about a 9\% reject rate on zipcode digits provided by the U.S. Postal Service.},
  booktitle = {Proceedings of the 3rd International Conference on Neural Information Processing Systems},
  pages     = {396–404},
  numpages  = {9},
  series    = {NIPS'89}
}

@inproceedings{kingma2017adammethodstochasticoptimization,
  title         = {Adam: A Method for Stochastic Optimization},
  author        = {Diederik P. Kingma and Jimmy Ba},
  booktitle     = {Proceedings of the 3rd International Conference on Learning Representations},
  year          = {2015},
  eprint        = {1412.6980},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1412.6980}
}

@inproceedings{he2015deepresiduallearningimage,
  title     = {Deep Residual Learning for Image Recognition},
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016},
  pages     = {770--778},
  doi       = {10.1109/CVPR.2016.90}
}

@article{hastie2020surpriseshighdimensionalridgelesssquares,
  title   = {Surprises in High-Dimensional Ridgeless Least Squares Interpolation},
  author  = {Trevor Hastie and Andrea Montanari and Saharon Rosset and Ryan J. Tibshirani},
  journal = {The Annals of Statistics},
  year    = {2022},
  volume  = {50},
  number  = {2},
  pages   = {949--986},
  doi     = {10.1214/21-AOS2133}
}

@inproceedings{Jastrzebski2020CatastrophicFE,
  title     = {Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization},
  author    = {Stanislaw Jastrzebski and Devansh Arpit and Oliver {\AA}strand and Giancarlo Kerg and Huan Wang and Caiming Xiong and Richard Socher and Kyunghyun Cho and Krzysztof J. Geras},
  booktitle = {International Conference on Machine Learning},
  year      = {2020},
  url       = {https://api.semanticscholar.org/CorpusID:229680169}
}

@inproceedings{boes1996dynamics,
  author    = {Boes, Siegfried and Opper, Manfred},
  title     = {Dynamics of Training},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {9},
  publisher = {MIT Press},
  year      = {1996},
  pages     = {141--147},
  url       = {https://papers.nips.cc/paper_files/paper/1996/hash/25df35de87aa441b88f22a6c2a830a17-Abstract.html}
}

@article{lecun1998gradientbasedlearningapplieddocumentrecognition,
  author   = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal  = {Proceedings of the IEEE},
  title    = {Gradient-based learning applied to document recognition},
  year     = {1998},
  volume   = {86},
  number   = {11},
  pages    = {2278-2324},
  keywords = {Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi      = {10.1109/5.726791}
}

@inproceedings{glorot2010understandingdifficultytrainingdeepfeedforward,
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  author    = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages     = {249--256},
  year      = {2010},
  editor    = {Teh, Yee Whye and Titterington, Mike},
  volume    = {9},
  series    = {Proceedings of Machine Learning Research},
  address   = {Chia Laguna Resort, Sardinia, Italy},
  month     = {13--15 May},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url       = {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract  = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@inproceedings{schulman2016high,
  title     = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  author    = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
  booktitle = {International Conference on Learning Representations},
  year      = {2016}
}

@inproceedings{gunasekar2017implicitregularizationmatrixfactorization,
  author    = {Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  title     = {Implicit regularization in matrix factorization},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix X with gradient descent on a factorization of X. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {6152–6160},
  numpages  = {9},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@misc{zhang2018studyoverfittingdeepreinforcement,
  title         = {A Study on Overfitting in Deep Reinforcement Learning},
  author        = {Chiyuan Zhang and Oriol Vinyals and Remi Munos and Samy Bengio},
  year          = {2018},
  eprint        = {1804.06893},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1804.06893}
}

@inproceedings{li2018algorithmicregularizationoverparameterized,
  title     = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
  author    = {Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle = {Proceedings of the 31st  Conference On Learning Theory},
  pages     = {2--47},
  year      = {2018},
  editor    = {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume    = {75},
  series    = {Proceedings of Machine Learning Research},
  month     = {06--09 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v75/li18a/li18a.pdf},
  url       = {https://proceedings.mlr.press/v75/li18a.html},
  abstract  = {We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. Concretely, we show that given $\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\star}$, we can recover $X^{\star}$ by parameterizing it by $UU^\top$ with $U\in \mathbb R^{d\times d}$ and minimizing the squared loss, even if $r \ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\star}$ in $\tilde{O}(\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.’17 under the restricted isometry property.  The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications.}
}

@article{Belkin_2019,
  title     = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
  volume    = {116},
  issn      = {1091-6490},
  url       = {http://dx.doi.org/10.1073/pnas.1903070116},
  doi       = {10.1073/pnas.1903070116},
  number    = {32},
  journal   = {Proceedings of the National Academy of Sciences},
  publisher = {Proceedings of the National Academy of Sciences},
  author    = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year      = {2019},
  month     = jul,
  pages     = {15849–15854}
}

@inproceedings{cobbe2019quantifyinggeneralizationreinforcement,
  title     = {Quantifying Generalization in Reinforcement Learning},
  author    = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {1282--1289},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/cobbe19a/cobbe19a.pdf},
  url       = {https://proceedings.mlr.press/v97/cobbe19a.html},
  abstract  = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent’s ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.}
}

@misc{nakkiran2019datahurtlinearregression,
  title         = {More Data Can Hurt for Linear Regression: Sample-wise Double Descent},
  author        = {Preetum Nakkiran},
  year          = {2019},
  eprint        = {1912.07242},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1912.07242}
}

@article{loog2020briefprehistorydoubledescent,
  author  = {Marco Loog and Tom Viering and Alexander Mey and Jesse H. Krijthe and David M. J. Tax},
  title   = {A brief prehistory of double descent},
  journal = {Proceedings of the National Academy of Sciences},
  volume  = {117},
  number  = {20},
  pages   = {10625-10626},
  year    = {2020},
  doi     = {10.1073/pnas.2001875117},
  url     = {https://www.pnas.org/doi/abs/10.1073/pnas.2001875117},
  eprint  = {https://www.pnas.org/doi/pdf/10.1073/pnas.2001875117}
}

@article{dAscoli2021TripleDescent,
  author  = {d'Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio},
  title   = {Triple descent and the two kinds of overfitting: where and why do they appear?},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  year    = {2021},
  volume  = {2021},
  number  = {12},
  pages   = {124002},
  doi     = {10.1088/1742-5468/ac3909}
}

@article{Buschjger2021ThereIsNoDoubleDescentRandomForests,
  title   = {There is no Double-Descent in Random Forests},
  author  = {Sebastian Buschj{\"a}ger and Katharina Morik},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2111.04409},
  url     = {https://api.semanticscholar.org/CorpusID:243848097}
}

@inproceedings{curth2023uturndoubledescent,
  author    = {Curth, Alicia and Jeffares, Alan and van der Schaar, Mihaela},
  title     = {A U-turn on double descent: rethinking parameter counting in statistical learning},
  year      = {2023},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a U-shaped curve reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count p grows past sample size n - a phenomenon dubbed double descent. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include linear regression, trees, and boosting. In this work, we take a closer look at the evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to what is being plotted on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple, distinct complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and only) when and where the transition between these underlying axes occurs, and that its location is thus not inherently tied to the interpolation threshold p = n. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as smoothers and propose a generalized measure for the effective number of parameters they use on unseen examples, using which we find that their apparent double descent curves do indeed fold back into more traditional convex shapes - providing a resolution to the ostensible tension between double descent and traditional statistical intuition.},
  booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
  articleno = {2439},
  numpages  = {31},
  location  = {New Orleans, LA, USA},
  series    = {NIPS '23}
}

@inproceedings{mediratta2024gengap,
  title     = {The Generalization Gap in Offline Reinforcement Learning},
  author    = {Ishita Mediratta and Qingfei You and Minqi Jiang and Roberta Raileanu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024},
  url       = {https://openreview.net/forum?id=3w6xuXDOdY}
}

@misc{falzari2025fisherguidedselectiveforgettingmitigating,
  title         = {Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep Reinforcement Learning},
  author        = {Massimiliano Falzari and Matthia Sabatelli},
  year          = {2025},
  eprint        = {2502.00802},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2502.00802}
}

@article{todorov2025sparsitydrivenplasticitymultitaskreinforcement,
  title   = {Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning},
  author  = {Aleksandar Todorov and Juan Cardenas-Cartagena and Rafael F. Cunha and Marco Zullich and Matthia Sabatelli},
  journal = {Transactions on Machine Learning Research},
  issn    = {2835-8856},
  year    = {2025},
  url     = {https://openreview.net/forum?id=9L4Z23EfE9},
  note    = {}
}